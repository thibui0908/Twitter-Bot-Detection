{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHc0BvZ8wcnC",
        "outputId": "569963c2-2c5d-4df6-c632-2894db2cb9fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Twitter-Bot-Detection'...\n",
            "remote: Enumerating objects: 5423, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 5423 (delta 3), reused 7 (delta 2), pack-reused 5412\u001b[K\n",
            "Receiving objects: 100% (5423/5423), 1.66 GiB | 27.21 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (2677/2677), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/thibui0908/Twitter-Bot-Detection.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlHkJsFlwraT"
      },
      "outputs": [],
      "source": [
        "## Vertex & Graph class\n",
        "\n",
        "class Vertex:\n",
        "    def __init__(self, key):\n",
        "        self.id = key\n",
        "        self.connectedTo = {}\n",
        "\n",
        "    def addNeighbor(self, neighbor, weight = 0):\n",
        "        self.connectedTo[neighbor] = weight\n",
        "    \n",
        "    def __str__(self):\n",
        "        return str(self.id) + ' connectedTo: ' + str([x.id for x in self.connectedTo])\n",
        "    \n",
        "    def getConnections(self):\n",
        "        return self.connectedTo.keys()\n",
        "    \n",
        "    def getId(self):\n",
        "        return self.id\n",
        "    \n",
        "    def getWeight(self, neighbor):\n",
        "        return self.connectedTo[neighbor]\n",
        "    \n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.vertexList = {}\n",
        "        self.numOfVertices = 0\n",
        "    \n",
        "    def addVertex(self, key):\n",
        "        self.numOfVertices = self.numOfVertices + 1\n",
        "        newVertex = Vertex(key)\n",
        "        self.vertexList[key] = newVertex\n",
        "        return newVertex\n",
        "\n",
        "    def getVertex(self, n):\n",
        "        if n in self.vertexList:\n",
        "            return self.vertexList[n]\n",
        "        else:\n",
        "            return None\n",
        "    \n",
        "    def __contains__(self, n):\n",
        "        return n in self.vertexList\n",
        "    \n",
        "    def addEdge(self, f, t, cost = 0):\n",
        "        if f not in self.vertexList:\n",
        "            nv = self.addVertex(f)\n",
        "        if t not in self.vertexList:\n",
        "            nv = self.addVertex(t)\n",
        "        self.vertexList[f].addNeighbor(self.vertexList[t], cost)\n",
        "    \n",
        "    def getVertices(self):\n",
        "        return self.vertexList.keys()\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return iter(self.vertexList.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WODmJ5NbxGZI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkixx3rKxXQ_",
        "outputId": "914a4d63-c992-42c4-d2e9-e8664f4fe3b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_data  Twitter-Bot-Detection\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7H2H8v_xTfm",
        "outputId": "d362350e-d8d8-43eb-b666-d411f91bc6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "568\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0                593661392\n",
              "1       891357264086528002\n",
              "2      1265307045160538116\n",
              "3                883085263\n",
              "4      1274567819225632769\n",
              "              ...         \n",
              "563    1240687632675409929\n",
              "564     811576970303705088\n",
              "565    1251905803839307777\n",
              "566              343717073\n",
              "567              233657084\n",
              "Name: Id, Length: 568, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nodes = pd.read_csv('Twitter-Bot-Detection/data/bot_user_nodes/bot_user_nodes_176.csv', names = ['Id'],header=None)\n",
        "nodes = nodes['Id']\n",
        "num_nodes = nodes.shape[0]\n",
        "print(num_nodes)\n",
        "nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "r2xhZAQByNTk",
        "outputId": "fd5ca522-c369-40f0-b292-03bc26f3d739"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c5217908-1e49-4ce9-a7d6-25293dac3d03\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>1576229254085345281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>3029183129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>1309466216860397568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>540018030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>783315967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7909</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>2611447940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7910</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>1279183267313393665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7911</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>1251905803839307777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7912</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>2445671278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7913</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>1265069381974994945</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7914 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5217908-1e49-4ce9-a7d6-25293dac3d03')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5217908-1e49-4ce9-a7d6-25293dac3d03 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5217908-1e49-4ce9-a7d6-25293dac3d03');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                   Source               Target\n",
              "0     1255066195235143681  1576229254085345281\n",
              "1     1255066195235143681           3029183129\n",
              "2     1255066195235143681  1309466216860397568\n",
              "3     1255066195235143681            540018030\n",
              "4     1255066195235143681            783315967\n",
              "...                   ...                  ...\n",
              "7909  1255066195235143681           2611447940\n",
              "7910  1255066195235143681  1279183267313393665\n",
              "7911  1255066195235143681  1251905803839307777\n",
              "7912  1255066195235143681           2445671278\n",
              "7913  1255066195235143681  1265069381974994945\n",
              "\n",
              "[7914 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edges = pd.read_csv('Twitter-Bot-Detection/data/bot_user_edge/bot_user_edge_176.csv', names = ['Source', 'Target'],header=None)\n",
        "edges.shape\n",
        "edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h7lPjlAyucs",
        "outputId": "6007d8a5-60b2-47e0-914f-8c59ef1c60a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 593661392-1255066195235143681\n",
            " 891357264086528002-1255066195235143681\n",
            " 1265307045160538116-1255066195235143681\n",
            " 883085263-1255066195235143681\n",
            " 1274567819225632769-1255066195235143681\n",
            " 1290085662511837185-1255066195235143681\n",
            " 1285575131906138113-1255066195235143681\n",
            " 1294905591387680768-1255066195235143681\n",
            " 1054333491410800640-1255066195235143681\n",
            " 1273015990733897735-1255066195235143681\n",
            " 1170914522879512577-1255066195235143681\n",
            " 1267442046291279872-1255066195235143681\n",
            " 1251566172627222536-1255066195235143681\n",
            " 1296014795687092224-1255066195235143681\n",
            " 1267078237655633920-1255066195235143681\n",
            " 1228663880202760192-1255066195235143681\n",
            " 1176476897514942464-1255066195235143681\n",
            " 1260462833197887490-1255066195235143681\n",
            " 1256400086881505281-1255066195235143681\n",
            " 1182576210016178176-1255066195235143681\n",
            " 1294786235546963969-1255066195235143681\n",
            " 1098713131914874880-1255066195235143681\n",
            " 1295639404065816576-1255066195235143681\n",
            " 137875162-1255066195235143681\n",
            " 1292829783232000001-1255066195235143681\n",
            " 1286658926423863296-1255066195235143681\n",
            " 1295656286428815361-1255066195235143681\n",
            " 1278155078596206594-1255066195235143681\n",
            " 1291126691839041536-1255066195235143681\n",
            " 1295401016318361600-1255066195235143681\n",
            " 1286593400024727554-1255066195235143681\n",
            " 1295328042684907520-1255066195235143681\n",
            " 1310909508097576961-1255066195235143681\n",
            " 1273662265649225728-1255066195235143681\n",
            " 1285713520315830274-1255066195235143681\n",
            " 1295186193617793024-1255066195235143681\n",
            " 1293990480750026752-1255066195235143681\n",
            " 1297149535022612480-1255066195235143681\n",
            " 856278458971934721-1255066195235143681\n",
            " 1288139359368228864-1255066195235143681\n",
            " 1295288932662489088-1255066195235143681\n",
            " 1296570952063561728-1255066195235143681\n",
            " 1270961413763276800-1255066195235143681\n",
            " 1292325099115122688-1255066195235143681\n",
            " 1189556819150229505-1255066195235143681\n",
            " 1284583107073650690-1255066195235143681\n",
            " 1286402700436013057-1255066195235143681\n",
            " 1295775386815680514-1255066195235143681\n",
            " 1251639904741871616-1255066195235143681\n",
            " 970530199698845696-1255066195235143681\n",
            " 1191038898077622272-1255066195235143681\n",
            " 162496998-1255066195235143681\n",
            " 1251139882707087360-1255066195235143681\n",
            " 1294332972724822025-1255066195235143681\n",
            " 1296410751800946690-1255066195235143681\n",
            " 1309466216860397568-1255066195235143681\n",
            " 1182480980583571456-1255066195235143681\n",
            " 1263065429213356032-1255066195235143681\n",
            " 1294963499182039046-1255066195235143681\n",
            " 2922998511-1255066195235143681\n",
            " 2611447940-1255066195235143681\n",
            " 1173709811722149900-1255066195235143681\n",
            " 1285759048999198720-1255066195235143681\n",
            " 1281899229972242433-1255066195235143681\n",
            " 1272935874486026240-1255066195235143681\n",
            " 1289133633253646337-1255066195235143681\n",
            " 1279078202703515650-1255066195235143681\n",
            " 1294140581518741504-1255066195235143681\n",
            " 1289241896884281344-1255066195235143681\n",
            " 1292431868411904001-1255066195235143681\n",
            " 1293055303534161920-1255066195235143681\n",
            " 889506948709621760-1255066195235143681\n",
            " 1296213870101639173-1255066195235143681\n",
            " 1205982204796514306-1255066195235143681\n",
            " 1148300187934318592-1255066195235143681\n",
            " 52848279-1255066195235143681\n",
            " 3366677945-1255066195235143681\n",
            " 1295012722371956743-1255066195235143681\n",
            " 1100683645419966464-1255066195235143681\n",
            " 1260184075001827329-1255066195235143681\n",
            " 2431258397-1255066195235143681\n",
            " 1292932077957120003-1255066195235143681\n",
            " 1295742734095933441-1255066195235143681\n",
            " 757103710623399936-1255066195235143681\n",
            " 1280098454975512576-1255066195235143681\n",
            " 1254864303045390338-1255066195235143681\n",
            " 1281592602962407432-1255066195235143681\n",
            " 1284106293591134210-1255066195235143681\n",
            " 1269585639600783361-1255066195235143681\n",
            " 1292098776841302017-1255066195235143681\n",
            " 1296181190177808385-1255066195235143681\n",
            " 1105805803720712192-1255066195235143681\n",
            " 1289105055812001796-1255066195235143681\n",
            " 1294905514749562882-1255066195235143681\n",
            " 796922334191976449-1255066195235143681\n",
            " 1294196871091625984-1255066195235143681\n",
            " 1294905363251236864-1255066195235143681\n",
            " 2485019736-1255066195235143681\n",
            " 1275882953105604609-1255066195235143681\n",
            " 1296430143385337862-1255066195235143681\n",
            " 1295067270654447616-1255066195235143681\n",
            " 1236447679586144258-1255066195235143681\n",
            " 1293217432971157504-1255066195235143681\n",
            " 1185576614706462725-1255066195235143681\n",
            " 1295002545493082112-1255066195235143681\n",
            " 1291302623988133888-1255066195235143681\n",
            " 1288580434956300295-1255066195235143681\n",
            " 1292991526310760448-1255066195235143681\n",
            " 1293280406285037570-1255066195235143681\n",
            " 1278570500923457540-1255066195235143681\n",
            " 153297386-1255066195235143681\n",
            " 1294950452287533056-1255066195235143681\n",
            " 1293038860839145473-1255066195235143681\n",
            " 1270500146946674693-1255066195235143681\n",
            " 1104584071873732608-1255066195235143681\n",
            " 1271512871718002689-1255066195235143681\n",
            " 1295084399944491008-1255066195235143681\n",
            " 1269337886807203842-1255066195235143681\n",
            " 1262365528850653190-1255066195235143681\n",
            " 1166334228310700032-1255066195235143681\n",
            " 1278418445319757824-1255066195235143681\n",
            " 884520190347796481-1255066195235143681\n",
            " 1277264679342137347-1255066195235143681\n",
            " 1128383675517886464-1255066195235143681\n",
            " 1285703698237345793-1255066195235143681\n",
            " 320896574-1255066195235143681\n",
            " 1285250843193880579-1255066195235143681\n",
            " 1269610079768260613-1255066195235143681\n",
            " 1576229254085345281-1255066195235143681\n",
            " 1290358843927605254-1255066195235143681\n",
            " 780453180652003328-1255066195235143681\n",
            " 1258502429768847360-1255066195235143681\n",
            " 732035208539701248-1255066195235143681\n",
            " 1221812240149409794-1255066195235143681\n",
            " 1156146482376073217-1255066195235143681\n",
            " 1294366866278842369-1255066195235143681\n",
            " 1291392454558732288-1255066195235143681\n",
            " 1295573965100986368-1255066195235143681\n",
            " 1295811402331049985-1255066195235143681\n",
            " 1291095492315828224-1255066195235143681\n",
            " 2192793307-1255066195235143681\n",
            " 1289965818294546432-1255066195235143681\n",
            " 841987065722109952-1255066195235143681\n",
            " 224057938-1255066195235143681\n",
            " 1235964683900813314-1255066195235143681\n",
            " 1296209668373700611-1255066195235143681\n",
            " 790761980286103552-1255066195235143681\n",
            " 1295522645019164672-1255066195235143681\n",
            " 1292240096352178177-1255066195235143681\n",
            " 1286763535159169024-1255066195235143681\n",
            " 1290634426288939009-1255066195235143681\n",
            " 1279183267313393665-1255066195235143681\n",
            " 1259858634881871875-1255066195235143681\n",
            " 2331322973-1255066195235143681\n",
            " 1288775292446744576-1255066195235143681\n",
            " 1271128841025658882-1255066195235143681\n",
            " 1292883743343075336-1255066195235143681\n",
            " 1296667948975677441-1255066195235143681\n",
            " 1259687542187790337-1255066195235143681\n",
            " 1259890341081120768-1255066195235143681\n",
            " 1292022383658840067-1255066195235143681\n",
            " 1296770896066220032-1255066195235143681\n",
            " 2518304822-1255066195235143681\n",
            " 947886318557782017-1255066195235143681\n",
            " 1293542139377455105-1255066195235143681\n",
            " 1288087966292869121-1255066195235143681\n",
            " 1294541112149975040-1255066195235143681\n",
            " 1288932595166859266-1255066195235143681\n",
            " 1295046905215569923-1255066195235143681\n",
            " 1294948617741115394-1255066195235143681\n",
            " 2196309060-1255066195235143681\n",
            " 1289983947666010112-1255066195235143681\n",
            " 900342825417658368-1255066195235143681\n",
            " 1271250689478529025-1255066195235143681\n",
            " 1292475511944417283-1255066195235143681\n",
            " 1294954869783060481-1255066195235143681\n",
            " 1294864347504705536-1255066195235143681\n",
            " 1280473514651377665-1255066195235143681\n",
            " 1214461370-1255066195235143681\n",
            " 1294869534650793985-1255066195235143681\n",
            " 1255066195235143681-1576229254085345281\n",
            " 1255066195235143681-3029183129\n",
            " 1255066195235143681-1309466216860397568\n",
            " 1255066195235143681-540018030\n",
            " 1255066195235143681-783315967\n",
            " 1255066195235143681-845491849938817024\n",
            " 1255066195235143681-1254864303045390338\n",
            " 1255066195235143681-1216436210983473152\n",
            " 1255066195235143681-1315436295720665088\n",
            " 1255066195235143681-1243457146811006976\n",
            " 1255066195235143681-1294342054185537537\n",
            " 1255066195235143681-1310909508097576961\n",
            " 1255066195235143681-1310643125875077121\n",
            " 1255066195235143681-1281911621577781250\n",
            " 1255066195235143681-1317891315296776193\n",
            " 1255066195235143681-790761980286103552\n",
            " 1255066195235143681-2485019736\n",
            " 1255066195235143681-884520190347796481\n",
            " 1255066195235143681-1098713131914874880\n",
            " 1255066195235143681-1289118393396678656\n",
            " 1255066195235143681-1189556819150229505\n",
            " 1255066195235143681-1303713883652321281\n",
            " 1255066195235143681-1228663880202760192\n",
            " 1255066195235143681-1278295442070089729\n",
            " 1255066195235143681-1208422854661853184\n",
            " 1255066195235143681-1173709811722149900\n",
            " 1255066195235143681-2605891879\n",
            " 1255066195235143681-1290890212206211072\n",
            " 1255066195235143681-1295573965100986368\n",
            " 1255066195235143681-1294332972724822025\n",
            " 1255066195235143681-1293707185642512385\n",
            " 1255066195235143681-1292098776841302017\n",
            " 1255066195235143681-823665462\n",
            " 1255066195235143681-1296597265461776391\n",
            " 1255066195235143681-1296213870101639173\n",
            " 1255066195235143681-1296914860517851136\n",
            " 1255066195235143681-1293410621741039627\n",
            " 1255066195235143681-970530199698845696\n",
            " 1255066195235143681-1297161318873600001\n",
            " 1255066195235143681-1297149535022612480\n",
            " 1255066195235143681-1296838540190461952\n",
            " 1255066195235143681-1261886743323738113\n",
            " 1255066195235143681-1296809057945296896\n",
            " 1255066195235143681-1297099149452759040\n",
            " 1255066195235143681-1275509022326771717\n",
            " 1255066195235143681-367270759\n",
            " 1255066195235143681-1291014372391890948\n",
            " 1255066195235143681-249109690\n",
            " 1255066195235143681-757103710623399936\n",
            " 1255066195235143681-1263600415645425666\n",
            " 1255066195235143681-1260184075001827329\n",
            " 1255066195235143681-1260531218820587520\n",
            " 1255066195235143681-1296389309352771584\n",
            " 1255066195235143681-1296211348968415233\n",
            " 1255066195235143681-1297054040799535104\n",
            " 1255066195235143681-1296432061201358853\n",
            " 1255066195235143681-1294905363251236864\n",
            " 1255066195235143681-89348893\n",
            " 1255066195235143681-1214461370\n",
            " 1255066195235143681-1291081887608377345\n",
            " 1255066195235143681-1283765614457356289\n",
            " 1255066195235143681-1294547899884478466\n",
            " 1255066195235143681-1291943250706812929\n",
            " 1255066195235143681-1156146482376073217\n",
            " 1255066195235143681-1294164743842930689\n",
            " 1255066195235143681-1264986428540321792\n",
            " 1255066195235143681-1281655931139436546\n",
            " 1255066195235143681-844384630899970048\n",
            " 1255066195235143681-1292022383658840067\n",
            " 1255066195235143681-840305359952719872\n",
            " 1255066195235143681-1296650728929460225\n",
            " 1255066195235143681-1296873785518428162\n",
            " 1255066195235143681-1260866619376644097\n",
            " 1255066195235143681-1291896749267783681\n",
            " 1255066195235143681-1285575131906138113\n",
            " 1255066195235143681-1111877962020851712\n",
            " 1255066195235143681-1272661718016110604\n",
            " 1255066195235143681-1295432970510245891\n",
            " 1255066195235143681-1288542941141708800\n",
            " 1255066195235143681-896196359845490688\n",
            " 1255066195235143681-1100683645419966464\n",
            " 1255066195235143681-1296770896066220032\n",
            " 1255066195235143681-1295046905215569923\n",
            " 1255066195235143681-1291688999069708288\n",
            " 1255066195235143681-998263517500362757\n",
            " 1255066195235143681-1259890341081120768\n",
            " 1255066195235143681-1280098454975512576\n",
            " 1255066195235143681-1258502429768847360\n",
            " 1255066195235143681-1251139882707087360\n",
            " 1255066195235143681-1291734255588933632\n",
            " 1255066195235143681-1296818449801502721\n",
            " 1255066195235143681-1296774895402188806\n",
            " 1255066195235143681-1288969280307027968\n",
            " 1255066195235143681-1291539303277957125\n",
            " 1255066195235143681-1291416785728798721\n",
            " 1255066195235143681-1296570952063561728\n",
            " 1255066195235143681-1054333491410800640\n",
            " 1255066195235143681-970999595403706368\n",
            " 1255066195235143681-1157836491126640640\n",
            " 1255066195235143681-1296667948975677441\n",
            " 1255066195235143681-1223107615074336774\n",
            " 1255066195235143681-1295984817767624705\n",
            " 1255066195235143681-780815146050473984\n",
            " 1255066195235143681-233240604\n",
            " 1255066195235143681-224057938\n",
            " 1255066195235143681-1296430143385337862\n",
            " 1255066195235143681-1284583107073650690\n",
            " 1255066195235143681-1269610079768260613\n",
            " 1255066195235143681-1294549756589547521\n",
            " 1255066195235143681-1269329038352560129\n",
            " 1255066195235143681-1296053675878645761\n",
            " 1255066195235143681-1296552016089616384\n",
            " 1255066195235143681-1293131038072291329\n",
            " 1255066195235143681-1235964683900813314\n",
            " 1255066195235143681-1293217432971157504\n",
            " 1255066195235143681-1288980319882878977\n",
            " 1255066195235143681-1294082803227860992\n",
            " 1255066195235143681-1296410751800946690\n",
            " 1255066195235143681-1294384953912971266\n",
            " 1255066195235143681-1296181190177808385\n",
            " 1255066195235143681-1294366866278842369\n",
            " 1255066195235143681-1152847046413434882\n",
            " 1255066195235143681-1292585230327480320\n",
            " 1255066195235143681-1288932595166859266\n",
            " 1255066195235143681-1288775292446744576\n",
            " 1255066195235143681-1295667633036156929\n",
            " 1255066195235143681-1288826381737000960\n",
            " 1255066195235143681-1286381392574783489\n",
            " 1255066195235143681-1246806675564175361\n",
            " 1255066195235143681-1221812240149409794\n",
            " 1255066195235143681-940665702515724289\n",
            " 1255066195235143681-1286593400024727554\n",
            " 1255066195235143681-1296189929245020162\n",
            " 1255066195235143681-1286764709010604032\n",
            " 1255066195235143681-1295143423054405638\n",
            " 1255066195235143681-1288906769692086276\n",
            " 1255066195235143681-1294962865456242689\n",
            " 1255066195235143681-1296041099216265217\n",
            " 1255066195235143681-1176476897514942464\n",
            " 1255066195235143681-1285576150530568194\n",
            " 1255066195235143681-2319513478\n",
            " 1255066195235143681-1291112582913417217\n",
            " 1255066195235143681-1296014795687092224\n",
            " 1255066195235143681-1293055303534161920\n",
            " 1255066195235143681-1275882953105604609\n",
            " 1255066195235143681-1279078202703515650\n",
            " 1255066195235143681-153297386\n",
            " 1255066195235143681-1269815170076942338\n",
            " 1255066195235143681-1280704651\n",
            " 1255066195235143681-856278458971934721\n",
            " 1255066195235143681-1248102552975093760\n",
            " 1255066195235143681-1278905706293444609\n",
            " 1255066195235143681-80378171\n",
            " 1255066195235143681-1296209668373700611\n",
            " 1255066195235143681-1293542139377455105\n",
            " 1255066195235143681-1296205370097049601\n",
            " 1255066195235143681-2431258397\n",
            " 1255066195235143681-3316000068\n",
            " 1255066195235143681-1291355078641831941\n",
            " 1255066195235143681-1293301280589393921\n",
            " 1255066195235143681-1288580434956300295\n",
            " 1255066195235143681-1292240096352178177\n",
            " 1255066195235143681-1285703698237345793\n",
            " 1255066195235143681-1295186193617793024\n",
            " 1255066195235143681-1292829783232000001\n",
            " 1255066195235143681-1294938808639684608\n",
            " 1255066195235143681-4924197077\n",
            " 1255066195235143681-3031798399\n",
            " 1255066195235143681-883085263\n",
            " 1255066195235143681-1251581461469302784\n",
            " 1255066195235143681-1285224168095612928\n",
            " 1255066195235143681-1295827286646763526\n",
            " 1255066195235143681-1295096327328727041\n",
            " 1255066195235143681-4519834933\n",
            " 1255066195235143681-1273013188448137218\n",
            " 1255066195235143681-1221809726008197121\n",
            " 1255066195235143681-1295365300259610626\n",
            " 1255066195235143681-1289480688358625280\n",
            " 1255066195235143681-1295742734095933441\n",
            " 1255066195235143681-1295812781766320128\n",
            " 1255066195235143681-1295639404065816576\n",
            " 1255066195235143681-1240687632675409929\n",
            " 1255066195235143681-1244015722105364482\n",
            " 1255066195235143681-1272318503320555520\n",
            " 1255066195235143681-1283212821854072838\n",
            " 1255066195235143681-1267362187129085953\n",
            " 1255066195235143681-1291418678945370118\n",
            " 1255066195235143681-1295522645019164672\n",
            " 1255066195235143681-1156205960857190402\n",
            " 1255066195235143681-2593503403\n",
            " 1255066195235143681-885575271277109253\n",
            " 1255066195235143681-1064255141199708160\n",
            " 1255066195235143681-801597606\n",
            " 1255066195235143681-1269585639600783361\n",
            " 1255066195235143681-1292932077957120003\n",
            " 1255066195235143681-1182480980583571456\n",
            " 1255066195235143681-1289157424088317952\n",
            " 1255066195235143681-1281899229972242433\n",
            " 1255066195235143681-1270961413763276800\n",
            " 1255066195235143681-1294226275687714818\n",
            " 1255066195235143681-1073963157478141957\n",
            " 1255066195235143681-970760011352887296\n",
            " 1255066195235143681-593661392\n",
            " 1255066195235143681-1261458240946176000\n",
            " 1255066195235143681-2441048692\n",
            " 1255066195235143681-457038008\n",
            " 1255066195235143681-1328166662\n",
            " 1255066195235143681-1263065429213356032\n",
            " 1255066195235143681-1286430194253864962\n",
            " 1255066195235143681-1085464558855221250\n",
            " 1255066195235143681-1295475542838452224\n",
            " 1255066195235143681-1295811402331049985\n",
            " 1255066195235143681-2518304822\n",
            " 1255066195235143681-1289238319927308288\n",
            " 1255066195235143681-1290482531477753856\n",
            " 1255066195235143681-3182072648\n",
            " 1255066195235143681-4024000335\n",
            " 1255066195235143681-3033738670\n",
            " 1255066195235143681-2192793307\n",
            " 1255066195235143681-1279097824374915072\n",
            " 1255066195235143681-1289133633253646337\n",
            " 1255066195235143681-1292627770137346051\n",
            " 1255066195235143681-1295759684197023745\n",
            " 1255066195235143681-1251639904741871616\n",
            " 1255066195235143681-1292991526310760448\n",
            " 1255066195235143681-1185576614706462725\n",
            " 1255066195235143681-1267237611921432576\n",
            " 1255066195235143681-1269337886807203842\n",
            " 1255066195235143681-1295775386815680514\n",
            " 1255066195235143681-1291546071714467840\n",
            " 1255066195235143681-1278736654732271617\n",
            " 1255066195235143681-1269489711829200896\n",
            " 1255066195235143681-1294878958694141956\n",
            " 1255066195235143681-1291424500752547843\n",
            " 1255066195235143681-1205982204796514306\n",
            " 1255066195235143681-1104584071873732608\n",
            " 1255066195235143681-1295002545493082112\n",
            " 1255066195235143681-1280357966085046272\n",
            " 1255066195235143681-1294963499182039046\n",
            " 1255066195235143681-1278570500923457540\n",
            " 1255066195235143681-1293232474579963904\n",
            " 1255066195235143681-1136215211361177601\n",
            " 1255066195235143681-3297198346\n",
            " 1255066195235143681-1105805803720712192\n",
            " 1255066195235143681-1283113677353361408\n",
            " 1255066195235143681-1041064179350687744\n",
            " 1255066195235143681-1295707824056676352\n",
            " 1255066195235143681-1291009245731631105\n",
            " 1255066195235143681-1286658926423863296\n",
            " 1255066195235143681-1259687542187790337\n",
            " 1255066195235143681-2196309060\n",
            " 1255066195235143681-4846475141\n",
            " 1255066195235143681-2943218523\n",
            " 1255066195235143681-1290634426288939009\n",
            " 1255066195235143681-1272935874486026240\n",
            " 1255066195235143681-302704460\n",
            " 1255066195235143681-1275765683972902913\n",
            " 1255066195235143681-1290879156822081536\n",
            " 1255066195235143681-719578166029725697\n",
            " 1255066195235143681-1277697021353304065\n",
            " 1255066195235143681-1295656286428815361\n",
            " 1255066195235143681-1295406606797209601\n",
            " 1255066195235143681-1295644122284462081\n",
            " 1255066195235143681-1291095492315828224\n",
            " 1255066195235143681-1258713669933789184\n",
            " 1255066195235143681-1295098333007159303\n",
            " 1255066195235143681-1295288932662489088\n",
            " 1255066195235143681-1293837280465960960\n",
            " 1255066195235143681-2962051187\n",
            " 1255066195235143681-1290405176961105921\n",
            " 1255066195235143681-1281921703694741515\n",
            " 1255066195235143681-372745375\n",
            " 1255066195235143681-1295360814174605312\n",
            " 1255066195235143681-233657084\n",
            " 1255066195235143681-1278744329213935616\n",
            " 1255066195235143681-1294914623041425408\n",
            " 1255066195235143681-1283411688813998081\n",
            " 1255066195235143681-1238485100305752064\n",
            " 1255066195235143681-1236447679586144258\n",
            " 1255066195235143681-1290101334197374976\n",
            " 1255066195235143681-1289965818294546432\n",
            " 1255066195235143681-278002132\n",
            " 1255066195235143681-1272264158730489856\n",
            " 1255066195235143681-1290103735579017216\n",
            " 1255066195235143681-1251566172627222536\n",
            " 1255066195235143681-1292992751324323841\n",
            " 1255066195235143681-1290085662511837185\n",
            " 1255066195235143681-859438653243883520\n",
            " 1255066195235143681-1277264679342137347\n",
            " 1255066195235143681-1293424200636665858\n",
            " 1255066195235143681-1295067270654447616\n",
            " 1255066195235143681-1290077838725599232\n",
            " 1255066195235143681-2922998511\n",
            " 1255066195235143681-1294515149756399618\n",
            " 1255066195235143681-1290001460609126400\n",
            " 1255066195235143681-1289983947666010112\n",
            " 1255066195235143681-1245803781062832128\n",
            " 1255066195235143681-1290040018732556288\n",
            " 1255066195235143681-1289963867649515532\n",
            " 1255066195235143681-1262182714121359364\n",
            " 1255066195235143681-1287779713109327872\n",
            " 1255066195235143681-1288934032626122753\n",
            " 1255066195235143681-343717073\n",
            " 1255066195235143681-1265307045160538116\n",
            " 1255066195235143681-162348110\n",
            " 1255066195235143681-1294465396095975424\n",
            " 1255066195235143681-1286728432399912961\n",
            " 1255066195235143681-2726709424\n",
            " 1255066195235143681-1288139359368228864\n",
            " 1255066195235143681-1271512871718002689\n",
            " 1255066195235143681-891357264086528002\n",
            " 1255066195235143681-1256400086881505281\n",
            " 1255066195235143681-1262365528850653190\n",
            " 1255066195235143681-1295208907569352704\n",
            " 1255066195235143681-1295063341333983238\n",
            " 1255066195235143681-1293038860839145473\n",
            " 1255066195235143681-1292852818517295104\n",
            " 1255066195235143681-396664556\n",
            " 1255066195235143681-1289273069878616065\n",
            " 1255066195235143681-1295328042684907520\n",
            " 1255066195235143681-1293990480750026752\n",
            " 1255066195235143681-732035208539701248\n",
            " 1255066195235143681-1128383675517886464\n",
            " 1255066195235143681-1291381477540868096\n",
            " 1255066195235143681-1295356711734325249\n",
            " 1255066195235143681-1288810383311077377\n",
            " 1255066195235143681-1292139578317590528\n",
            " 1255066195235143681-1260626851623567364\n",
            " 1255066195235143681-1259858634881871875\n",
            " 1255066195235143681-133867558\n",
            " 1255066195235143681-1251234433006477312\n",
            " 1255066195235143681-1289241896884281344\n",
            " 1255066195235143681-1283039349521735685\n",
            " 1255066195235143681-1286974801261756416\n",
            " 1255066195235143681-163251697\n",
            " 1255066195235143681-320896574\n",
            " 1255066195235143681-930188199221170176\n",
            " 1255066195235143681-1293904826640863232\n",
            " 1255066195235143681-1293982355045134336\n",
            " 1255066195235143681-1290358843927605254\n",
            " 1255066195235143681-144734582\n",
            " 1255066195235143681-1268464648623476738\n",
            " 1255066195235143681-1290811466602381313\n",
            " 1255066195235143681-1294723486661980163\n",
            " 1255066195235143681-1270717189650214914\n",
            " 1255066195235143681-484239797\n",
            " 1255066195235143681-1295097085348458496\n",
            " 1255066195235143681-1292883743343075336\n",
            " 1255066195235143681-1223263451226812423\n",
            " 1255066195235143681-1182576210016178176\n",
            " 1255066195235143681-1399886575\n",
            " 1255066195235143681-1291774786939617286\n",
            " 1255066195235143681-1295401016318361600\n",
            " 1255066195235143681-1292475511944417283\n",
            " 1255066195235143681-1271157883670089728\n",
            " 1255066195235143681-1261654736673083394\n",
            " 1255066195235143681-1243159985087025152\n",
            " 1255066195235143681-1290089742172815360\n",
            " 1255066195235143681-1294834482948837377\n",
            " 1255066195235143681-1288087966292869121\n",
            " 1255066195235143681-1291392454558732288\n",
            " 1255066195235143681-1166334228310700032\n",
            " 1255066195235143681-1289332038600925184\n",
            " 1255066195235143681-900342825417658368\n",
            " 1255066195235143681-1292237677262188545\n",
            " 1255066195235143681-2431179918\n",
            " 1255066195235143681-1286763535159169024\n",
            " 1255066195235143681-1294869534650793985\n",
            " 1255066195235143681-579920865\n",
            " 1255066195235143681-1294952414953508865\n",
            " 1255066195235143681-1250989398323351554\n",
            " 1255066195235143681-1273662265649225728\n",
            " 1255066195235143681-1251986293237653505\n",
            " 1255066195235143681-1257859836336844806\n",
            " 1255066195235143681-1277446412410793985\n",
            " 1255066195235143681-138917350\n",
            " 1255066195235143681-1273015990733897735\n",
            " 1255066195235143681-1289221232777482243\n",
            " 1255066195235143681-1294679004608434176\n",
            " 1255066195235143681-1293487677552173057\n",
            " 1255066195235143681-889506948709621760\n",
            " 1255066195235143681-1280332791234822144\n",
            " 1255066195235143681-1294789348081180672\n",
            " 1255066195235143681-1271128841025658882\n",
            " 1255066195235143681-1260462833197887490\n",
            " 1255066195235143681-1270681767599640578\n",
            " 1255066195235143681-1295332345843716097\n",
            " 1255066195235143681-1269215111941586944\n",
            " 1255066195235143681-1291841966234312706\n",
            " 1255066195235143681-841987065722109952\n",
            " 1255066195235143681-1251230116744757250\n",
            " 1255066195235143681-304660050\n",
            " 1255066195235143681-811576970303705088\n",
            " 1255066195235143681-1291315182329880577\n",
            " 1255066195235143681-1284441556464939011\n",
            " 1255066195235143681-1289105055812001796\n",
            " 1255066195235143681-1280173683689107457\n",
            " 1255066195235143681-2331322973\n",
            " 1255066195235143681-1292431868411904001\n",
            " 1255066195235143681-1238268857355317253\n",
            " 1255066195235143681-1295319901826166784\n",
            " 1255066195235143681-1004379568105549825\n",
            " 1255066195235143681-1295320002682400769\n",
            " 1255066195235143681-1148300187934318592\n",
            " 1255066195235143681-1294278079414665218\n",
            " 1255066195235143681-1284106293591134210\n",
            " 1255066195235143681-1049771376805072905\n",
            " 1255066195235143681-1295059299622952960\n",
            " 1255066195235143681-1280037126881587200\n",
            " 1255066195235143681-1294950452287533056\n",
            " 1255066195235143681-1294842859930226689\n",
            " 1255066195235143681-1295154590401409025\n",
            " 1255066195235143681-1285348832247775239\n",
            " 1255066195235143681-1274499088470257664\n",
            " 1255066195235143681-1294954869783060481\n",
            " 1255066195235143681-219628940\n",
            " 1255066195235143681-1289243686879014913\n",
            " 1255066195235143681-3366677945\n",
            " 1255066195235143681-796922334191976449\n",
            " 1255066195235143681-1274567819225632769\n",
            " 1255066195235143681-1293602183636713472\n",
            " 1255066195235143681-1294567059599298561\n",
            " 1255066195235143681-1262956275962437637\n",
            " 1255066195235143681-1294196871091625984\n",
            " 1255066195235143681-1281135742169874432\n",
            " 1255066195235143681-1280473514651377665\n",
            " 1255066195235143681-989061568976642048\n",
            " 1255066195235143681-1291302623988133888\n",
            " 1255066195235143681-1295262606635274242\n",
            " 1255066195235143681-1291381378576191499\n",
            " 1255066195235143681-1270391351964729344\n",
            " 1255066195235143681-1282646769554399235\n",
            " 1255066195235143681-1271865381737705477\n",
            " 1255066195235143681-1256608274985730050\n",
            " 1255066195235143681-1294947497933262848\n",
            " 1255066195235143681-1279545213082570754\n",
            " 1255066195235143681-1295084399944491008\n",
            " 1255066195235143681-964986451208425472\n",
            " 1255066195235143681-1284032058264780800\n",
            " 1255066195235143681-1289556848866140161\n",
            " 1255066195235143681-1294905514749562882\n",
            " 1255066195235143681-1267971866360193027\n",
            " 1255066195235143681-1294878217275420672\n",
            " 1255066195235143681-338532028\n",
            " 1255066195235143681-1269538921093959682\n",
            " 1255066195235143681-1294840560067411970\n",
            " 1255066195235143681-1258590274156146688\n",
            " 1255066195235143681-1268880610573025281\n",
            " 1255066195235143681-52848279\n",
            " 1255066195235143681-1295212317110464513\n",
            " 1255066195235143681-1463533507\n",
            " 1255066195235143681-1295012722371956743\n",
            " 1255066195235143681-1285874413632552960\n",
            " 1255066195235143681-1294864347504705536\n",
            " 1255066195235143681-1285250843193880579\n",
            " 1255066195235143681-1267744996846788608\n",
            " 1255066195235143681-870147302\n",
            " 1255066195235143681-1294541112149975040\n",
            " 1255066195235143681-1294097874909831170\n",
            " 1255066195235143681-1294140581518741504\n",
            " 1255066195235143681-1170914522879512577\n",
            " 1255066195235143681-1283149515827159040\n",
            " 1255066195235143681-1287396182562672641\n",
            " 1255066195235143681-1233698303335895040\n",
            " 1255066195235143681-1284758989\n",
            " 1255066195235143681-1281250688631279616\n",
            " 1255066195235143681-1278155078596206594\n",
            " 1255066195235143681-1276598586147581952\n",
            " 1255066195235143681-1263340397939613697\n",
            " 1255066195235143681-1277572543285653505\n",
            " 1255066195235143681-1266504822150303746\n",
            " 1255066195235143681-1251840289032990720\n",
            " 1255066195235143681-1294125390462689280\n",
            " 1255066195235143681-1290862544412975105\n",
            " 1255066195235143681-1278418445319757824\n",
            " 1255066195235143681-4835742233\n",
            " 1255066195235143681-1274427490166022145\n",
            " 1255066195235143681-1272294283274604547\n",
            " 1255066195235143681-162496998\n",
            " 1255066195235143681-1288511059154948096\n",
            " 1255066195235143681-1285254169000062977\n",
            " 1255066195235143681-507908142\n",
            " 1255066195235143681-1281030577509011463\n",
            " 1255066195235143681-1292052696351100928\n",
            " 1255066195235143681-1287878225448837120\n",
            " 1255066195235143681-1294786235546963969\n",
            " 1255066195235143681-224027210\n",
            " 1255066195235143681-1292325099115122688\n",
            " 1255066195235143681-137875162\n",
            " 1255066195235143681-1142054241743384587\n",
            " 1255066195235143681-1266044609379160064\n",
            " 1255066195235143681-1292515499901886465\n",
            " 1255066195235143681-1291854055711547393\n",
            " 1255066195235143681-1294562494065893376\n",
            " 1255066195235143681-1290703276493746177\n",
            " 1255066195235143681-1291126691839041536\n",
            " 1255066195235143681-1290080661060149248\n",
            " 1255066195235143681-1281100470384287744\n",
            " 1255066195235143681-1258875447749812225\n",
            " 1255066195235143681-1277944293030838274\n",
            " 1255066195235143681-966308155029409793\n",
            " 1255066195235143681-1294905591387680768\n",
            " 1255066195235143681-62995990\n",
            " 1255066195235143681-1294228976773341184\n",
            " 1255066195235143681-1293189360288849923\n",
            " 1255066195235143681-1293470845684387840\n",
            " 1255066195235143681-1293657736492863488\n",
            " 1255066195235143681-1288407011533750272\n",
            " 1255066195235143681-1373333412\n",
            " 1255066195235143681-1294982934793838592\n",
            " 1255066195235143681-1285786578724556806\n",
            " 1255066195235143681-780453180652003328\n",
            " 1255066195235143681-1292568270520881153\n",
            " 1255066195235143681-767396675589578752\n",
            " 1255066195235143681-1269045865186017280\n",
            " 1255066195235143681-1280472250173534208\n",
            " 1255066195235143681-1285904125557985281\n",
            " 1255066195235143681-1291953688936943618\n",
            " 1255066195235143681-1294839984634068993\n",
            " 1255066195235143681-1294948617741115394\n",
            " 1255066195235143681-947886318557782017\n",
            " 1255066195235143681-1287888866427404289\n",
            " 1255066195235143681-1288087391241109504\n",
            " 1255066195235143681-1288150063747944448\n",
            " 1255066195235143681-1286402700436013057\n",
            " 1255066195235143681-1281592602962407432\n",
            " 1255066195235143681-1270387226640371713\n",
            " 1255066195235143681-1275247607238086656\n",
            " 1255066195235143681-1285713520315830274\n",
            " 1255066195235143681-1287792612599115776\n",
            " 1255066195235143681-1292528402809925633\n",
            " 1255066195235143681-1267442046291279872\n",
            " 1255066195235143681-1294020924631658496\n",
            " 1255066195235143681-1290059606731956224\n",
            " 1255066195235143681-1280916713031307264\n",
            " 1255066195235143681-1288271362411442177\n",
            " 1255066195235143681-1285544159089168384\n",
            " 1255066195235143681-1290082867352150018\n",
            " 1255066195235143681-1294837709903728640\n",
            " 1255066195235143681-1240037056597950464\n",
            " 1255066195235143681-1289957040375443457\n",
            " 1255066195235143681-1286296232374210567\n",
            " 1255066195235143681-1267078237655633920\n",
            " 1255066195235143681-1277670721385304065\n",
            " 1255066195235143681-1295000624078229504\n",
            " 1255066195235143681-1285759048999198720\n",
            " 1255066195235143681-1287670626665234434\n",
            " 1255066195235143681-1293280406285037570\n",
            " 1255066195235143681-1270500146946674693\n",
            " 1255066195235143681-1191038898077622272\n",
            " 1255066195235143681-1271250689478529025\n",
            " 1255066195235143681-169291350\n",
            " 1255066195235143681-1282666555499851777\n",
            " 1255066195235143681-1278683376304734213\n",
            " 1255066195235143681-1288162120790675456\n",
            " 1255066195235143681-1008723958311673857\n",
            " 1255066195235143681-1274765802156277760\n",
            " 1255066195235143681-1292832864367771650\n",
            " 1255066195235143681-986353306854395904\n",
            " 1255066195235143681-1289891905111822338\n",
            " 1255066195235143681-2611447940\n",
            " 1255066195235143681-1279183267313393665\n",
            " 1255066195235143681-1251905803839307777\n",
            " 1255066195235143681-2445671278\n",
            " 1255066195235143681-1265069381974994945\n",
            " 1255066195235143681-1248206149117149185\n",
            " 1255066195235143681-1283256522764681216\n",
            " 1274765802156277760-1255066195235143681\n",
            " 1267237611921432576-1255066195235143681\n",
            " 1296873785518428162-1255066195235143681\n",
            " 1271865381737705477-1255066195235143681\n",
            " 1280037126881587200-1255066195235143681\n",
            " 1265069381974994945-1255066195235143681\n",
            " 133867558-1255066195235143681\n",
            " 1287792612599115776-1255066195235143681\n",
            " 1272661718016110604-1255066195235143681\n",
            " 1285874413632552960-1255066195235143681\n",
            " 1293487677552173057-1255066195235143681\n",
            " 1293410621741039627-1255066195235143681\n",
            " 1238268857355317253-1255066195235143681\n",
            " 1270391351964729344-1255066195235143681\n",
            " 1295667633036156929-1255066195235143681\n",
            " 801597606-1255066195235143681\n",
            " 1238485100305752064-1255066195235143681\n",
            " 1281250688631279616-1255066195235143681\n",
            " 1288906769692086276-1255066195235143681\n",
            " 1288810383311077377-1255066195235143681\n",
            " 1290103735579017216-1255066195235143681\n",
            " 1294947497933262848-1255066195235143681\n",
            " 1258713669933789184-1255066195235143681\n",
            " 2319513478-1255066195235143681\n",
            " 1295360814174605312-1255066195235143681\n",
            " 989061568976642048-1255066195235143681\n",
            " 1284441556464939011-1255066195235143681\n",
            " 1288980319882878977-1255066195235143681\n",
            " 1260866619376644097-1255066195235143681\n",
            " 1283256522764681216-1255066195235143681\n",
            " 1273013188448137218-1255066195235143681\n",
            " 1291774786939617286-1255066195235143681\n",
            " 1277572543285653505-1255066195235143681\n",
            " 719578166029725697-1255066195235143681\n",
            " 1296809057945296896-1255066195235143681\n",
            " 844384630899970048-1255066195235143681\n",
            " 1283212821854072838-1255066195235143681\n",
            " 1290482531477753856-1255066195235143681\n",
            " 1270387226640371713-1255066195235143681\n",
            " 1294679004608434176-1255066195235143681\n",
            " 1296597265461776391-1255066195235143681\n",
            " 2445671278-1255066195235143681\n",
            " 1277697021353304065-1255066195235143681\n",
            " 1289221232777482243-1255066195235143681\n",
            " 1289891905111822338-1255066195235143681\n",
            " 1288271362411442177-1255066195235143681\n",
            " 1296818449801502721-1255066195235143681\n",
            " 1296041099216265217-1255066195235143681\n",
            " 89348893-1255066195235143681\n",
            " 1292627770137346051-1255066195235143681\n",
            " 1288087391241109504-1255066195235143681\n",
            " 2593503403-1255066195235143681\n",
            " 1289480688358625280-1255066195235143681\n",
            " 1294840560067411970-1255066195235143681\n",
            " 1297161318873600001-1255066195235143681\n",
            " 1296053675878645761-1255066195235143681\n",
            " 1073963157478141957-1255066195235143681\n",
            " 1258590274156146688-1255066195235143681\n",
            " 1290040018732556288-1255066195235143681\n",
            " 964986451208425472-1255066195235143681\n",
            " 1251581461469302784-1255066195235143681\n",
            " 3031798399-1255066195235143681\n",
            " 1296205370097049601-1255066195235143681\n",
            " 3297198346-1255066195235143681\n",
            " 1272264158730489856-1255066195235143681\n",
            " 1284758989-1255066195235143681\n",
            " 1296552016089616384-1255066195235143681\n",
            " 885575271277109253-1255066195235143681\n",
            " 484239797-1255066195235143681\n",
            " 1251234433006477312-1255066195235143681\n",
            " 1277944293030838274-1255066195235143681\n",
            " 540018030-1255066195235143681\n",
            " 1290811466602381313-1255066195235143681\n",
            " 1294834482948837377-1255066195235143681\n",
            " 1288969280307027968-1255066195235143681\n",
            " 1295812781766320128-1255066195235143681\n",
            " 1292515499901886465-1255066195235143681\n",
            " 1290077838725599232-1255066195235143681\n",
            " 1250989398323351554-1255066195235143681\n",
            " 224027210-1255066195235143681\n",
            " 1296838540190461952-1255066195235143681\n",
            " 338532028-1255066195235143681\n",
            " 1295096327328727041-1255066195235143681\n",
            " 1285904125557985281-1255066195235143681\n",
            " 2726709424-1255066195235143681\n",
            " 1293707185642512385-1255066195235143681\n",
            " 1279097824374915072-1255066195235143681\n",
            " 1251986293237653505-1255066195235143681\n",
            " 1282666555499851777-1255066195235143681\n",
            " 896196359845490688-1255066195235143681\n",
            " 1291009245731631105-1255066195235143681\n",
            " 162348110-1255066195235143681\n",
            " 1280916713031307264-1255066195235143681\n",
            " 1292832864367771650-1255066195235143681\n",
            " 1260626851623567364-1255066195235143681\n",
            " 1295143423054405638-1255066195235143681\n",
            " 1244015722105364482-1255066195235143681\n",
            " 1278295442070089729-1255066195235143681\n",
            " 1293131038072291329-1255066195235143681\n",
            " 396664556-1255066195235143681\n",
            " 1292237677262188545-1255066195235143681\n",
            " 1284032058264780800-1255066195235143681\n",
            " 1291734255588933632-1255066195235143681\n",
            " 1286430194253864962-1255066195235143681\n",
            " 1290082867352150018-1255066195235143681\n",
            " 507908142-1255066195235143681\n",
            " 823665462-1255066195235143681\n",
            " 1272294283274604547-1255066195235143681\n",
            " 1290001460609126400-1255066195235143681\n",
            " 1295059299622952960-1255066195235143681\n",
            " 1295000624078229504-1255066195235143681\n",
            " 249109690-1255066195235143681\n",
            " 1294842859930226689-1255066195235143681\n",
            " 1292992751324323841-1255066195235143681\n",
            " 1287888866427404289-1255066195235143681\n",
            " 1291381477540868096-1255066195235143681\n",
            " 1288407011533750272-1255066195235143681\n",
            " 1295063341333983238-1255066195235143681\n",
            " 1291546071714467840-1255066195235143681\n",
            " 2943218523-1255066195235143681\n",
            " 1295208907569352704-1255066195235143681\n",
            " 1296914860517851136-1255066195235143681\n",
            " 138917350-1255066195235143681\n",
            " 1296432061201358853-1255066195235143681\n",
            " 1288826381737000960-1255066195235143681\n",
            " 1294952414953508865-1255066195235143681\n",
            " 1294837709903728640-1255066195235143681\n",
            " 144734582-1255066195235143681\n",
            " 1278683376304734213-1255066195235143681\n",
            " 1291081887608377345-1255066195235143681\n",
            " 1290059606731956224-1255066195235143681\n",
            " 1283039349521735685-1255066195235143681\n",
            " 1269329038352560129-1255066195235143681\n",
            " 1152847046413434882-1255066195235143681\n",
            " 1233698303335895040-1255066195235143681\n",
            " 1285576150530568194-1255066195235143681\n",
            " 1291112582913417217-1255066195235143681\n",
            " 1294723486661980163-1255066195235143681\n",
            " 1278905706293444609-1255066195235143681\n",
            " 1293232474579963904-1255066195235143681\n",
            " 1064255141199708160-1255066195235143681\n",
            " 1290703276493746177-1255066195235143681\n",
            " 4519834933-1255066195235143681\n",
            " 1274499088470257664-1255066195235143681\n",
            " 1266504822150303746-1255066195235143681\n",
            " 4024000335-1255066195235143681\n",
            " 1156205960857190402-1255066195235143681\n",
            " 1297054040799535104-1255066195235143681\n",
            " 1290405176961105921-1255066195235143681\n",
            " 1291381378576191499-1255066195235143681\n",
            " 1288162120790675456-1255066195235143681\n",
            " 2431179918-1255066195235143681\n",
            " 1280704651-1255066195235143681\n",
            " 1292852818517295104-1255066195235143681\n",
            " 1267971866360193027-1255066195235143681\n",
            " 1463533507-1255066195235143681\n",
            " 1281911621577781250-1255066195235143681\n",
            " 1248102552975093760-1255066195235143681\n",
            " 1294226275687714818-1255066195235143681\n",
            " 1291416785728798721-1255066195235143681\n",
            " 1257859836336844806-1255066195235143681\n",
            " 1262182714121359364-1255066195235143681\n",
            " 1295475542838452224-1255066195235143681\n",
            " 1294562494065893376-1255066195235143681\n",
            " 1248206149117149185-1255066195235143681\n",
            " 1279545213082570754-1255066195235143681\n",
            " 1295644122284462081-1255066195235143681\n",
            " 1258875447749812225-1255066195235143681\n",
            " 1041064179350687744-1255066195235143681\n",
            " 1399886575-1255066195235143681\n",
            " 1288511059154948096-1255066195235143681\n",
            " 1303713883652321281-1255066195235143681\n",
            " 1310643125875077121-1255066195235143681\n",
            " 1294547899884478466-1255066195235143681\n",
            " 1291688999069708288-1255066195235143681\n",
            " 1270717189650214914-1255066195235143681\n",
            " 1293470845684387840-1255066195235143681\n",
            " 859438653243883520-1255066195235143681\n",
            " 1290089742172815360-1255066195235143681\n",
            " 1269489711829200896-1255066195235143681\n",
            " 1271157883670089728-1255066195235143681\n",
            " 1295320002682400769-1255066195235143681\n",
            " 1263340397939613697-1255066195235143681\n",
            " 1251840289032990720-1255066195235143681\n",
            " 1142054241743384587-1255066195235143681\n",
            " 1290879156822081536-1255066195235143681\n",
            " 1295759684197023745-1255066195235143681\n",
            " 278002132-1255066195235143681\n",
            " 1049771376805072905-1255066195235143681\n",
            " 1294789348081180672-1255066195235143681\n",
            " 1287396182562672641-1255066195235143681\n",
            " 1294465396095975424-1255066195235143681\n",
            " 1295365300259610626-1255066195235143681\n",
            " 1295707824056676352-1255066195235143681\n",
            " 457038008-1255066195235143681\n",
            " 1111877962020851712-1255066195235143681\n",
            " 1295319901826166784-1255066195235143681\n",
            " 1295984817767624705-1255066195235143681\n",
            " 3316000068-1255066195235143681\n",
            " 1268880610573025281-1255066195235143681\n",
            " 845491849938817024-1255066195235143681\n",
            " 1260531218820587520-1255066195235143681\n",
            " 1296650728929460225-1255066195235143681\n",
            " 62995990-1255066195235143681\n",
            " 1294082803227860992-1255066195235143681\n",
            " 1280357966085046272-1255066195235143681\n",
            " 940665702515724289-1255066195235143681\n",
            " 1288542941141708800-1255066195235143681\n",
            " 966308155029409793-1255066195235143681\n",
            " 1283113677353361408-1255066195235143681\n",
            " 1285224168095612928-1255066195235143681\n",
            " 1208422854661853184-1255066195235143681\n",
            " 1289243686879014913-1255066195235143681\n",
            " 3182072648-1255066195235143681\n",
            " 1295097085348458496-1255066195235143681\n",
            " 1289332038600925184-1255066195235143681\n",
            " 1261886743323738113-1255066195235143681\n",
            " 233240604-1255066195235143681\n",
            " 1283765614457356289-1255066195235143681\n",
            " 1291896749267783681-1255066195235143681\n",
            " 1291943250706812929-1255066195235143681\n",
            " 1283149515827159040-1255066195235143681\n",
            " 579920865-1255066195235143681\n",
            " 80378171-1255066195235143681\n",
            " 3029183129-1255066195235143681\n",
            " 870147302-1255066195235143681\n",
            " 1085464558855221250-1255066195235143681\n",
            " 970760011352887296-1255066195235143681\n",
            " 1245803781062832128-1255066195235143681\n",
            " 1286381392574783489-1255066195235143681\n",
            " 1275509022326771717-1255066195235143681\n",
            " 1270681767599640578-1255066195235143681\n",
            " 1286764709010604032-1255066195235143681\n",
            " 1294878217275420672-1255066195235143681\n",
            " 1285786578724556806-1255066195235143681\n",
            " 1294125390462689280-1255066195235143681\n",
            " 1290101334197374976-1255066195235143681\n",
            " 372745375-1255066195235143681\n",
            " 1296389309352771584-1255066195235143681\n",
            " 1291355078641831941-1255066195235143681\n",
            " 1287670626665234434-1255066195235143681\n",
            " 1295154590401409025-1255066195235143681\n",
            " 1286296232374210567-1255066195235143681\n",
            " 2962051187-1255066195235143681\n",
            " 1281100470384287744-1255066195235143681\n",
            " 1267362187129085953-1255066195235143681\n",
            " 1269045865186017280-1255066195235143681\n",
            " 1294549756589547521-1255066195235143681\n",
            " 1262956275962437637-1255066195235143681\n",
            " 767396675589578752-1255066195235143681\n",
            " 1294938808639684608-1255066195235143681\n",
            " 1223263451226812423-1255066195235143681\n",
            " 1293657736492863488-1255066195235143681\n",
            " 1243159985087025152-1255066195235143681\n",
            " 1281921703694741515-1255066195235143681\n",
            " 1328166662-1255066195235143681\n",
            " 1264986428540321792-1255066195235143681\n",
            " 1277670721385304065-1255066195235143681\n",
            " 1269815170076942338-1255066195235143681\n",
            " 304660050-1255066195235143681\n",
            " 1295356711734325249-1255066195235143681\n",
            " 1294164743842930689-1255066195235143681\n",
            " 4924197077-1255066195235143681\n",
            " 1293837280465960960-1255066195235143681\n",
            " 1276598586147581952-1255066195235143681\n",
            " 1292052696351100928-1255066195235143681\n",
            " 1278736654732271617-1255066195235143681\n",
            " 1294515149756399618-1255066195235143681\n",
            " 1240037056597950464-1255066195235143681\n",
            " 1292585230327480320-1255066195235143681\n",
            " 1266044609379160064-1255066195235143681\n",
            " 1317891315296776193-1255066195235143681\n",
            " 1288934032626122753-1255066195235143681\n",
            " 1269538921093959682-1255066195235143681\n",
            " 1261654736673083394-1255066195235143681\n",
            " 1286728432399912961-1255066195235143681\n",
            " 1293904826640863232-1255066195235143681\n",
            " 1296189929245020162-1255066195235143681\n",
            " 1293602183636713472-1255066195235143681\n",
            " 2441048692-1255066195235143681\n",
            " 1294020924631658496-1255066195235143681\n",
            " 1281030577509011463-1255066195235143681\n",
            " 1294342054185537537-1255066195235143681\n",
            " 1295212317110464513-1255066195235143681\n",
            " 1287878225448837120-1255066195235143681\n",
            " 4835742233-1255066195235143681\n",
            " 1292139578317590528-1255066195235143681\n",
            " 1294097874909831170-1255066195235143681\n",
            " 986353306854395904-1255066195235143681\n",
            " 1297099149452759040-1255066195235143681\n",
            " 1008723958311673857-1255066195235143681\n",
            " 1277446412410793985-1255066195235143681\n",
            " 1295827286646763526-1255066195235143681\n",
            " 169291350-1255066195235143681\n",
            " 1289957040375443457-1255066195235143681\n",
            " 1289273069878616065-1255066195235143681\n",
            " 1136215211361177601-1255066195235143681\n",
            " 1290890212206211072-1255066195235143681\n",
            " 1291854055711547393-1255066195235143681\n",
            " 1268464648623476738-1255066195235143681\n",
            " 1295262606635274242-1255066195235143681\n",
            " 1293189360288849923-1255066195235143681\n",
            " 1289963867649515532-1255066195235143681\n",
            " 1294567059599298561-1255066195235143681\n",
            " 1294982934793838592-1255066195235143681\n",
            " 1296774895402188806-1255066195235143681\n",
            " 1286974801261756416-1255066195235143681\n",
            " 1293424200636665858-1255066195235143681\n",
            " 1261458240946176000-1255066195235143681\n",
            " 1290080661060149248-1255066195235143681\n",
            " 1294962865456242689-1255066195235143681\n",
            " 1004379568105549825-1255066195235143681\n",
            " 1256608274985730050-1255066195235143681\n",
            " 1290862544412975105-1255066195235143681\n",
            " 1281135742169874432-1255066195235143681\n",
            " 1293982355045134336-1255066195235143681\n",
            " 1289118393396678656-1255066195235143681\n",
            " 1285348832247775239-1255066195235143681\n",
            " 1282646769554399235-1255066195235143681\n",
            " 1315436295720665088-1255066195235143681\n",
            " 970999595403706368-1255066195235143681\n",
            " 1223107615074336774-1255066195235143681\n",
            " 1280173683689107457-1255066195235143681\n",
            " 1285544159089168384-1255066195235143681\n",
            " 302704460-1255066195235143681\n",
            " 1251230116744757250-1255066195235143681\n",
            " 1280472250173534208-1255066195235143681\n",
            " 1283411688813998081-1255066195235143681\n",
            " 1281655931139436546-1255066195235143681\n",
            " 1275765683972902913-1255066195235143681\n",
            " 3033738670-1255066195235143681\n",
            " 1157836491126640640-1255066195235143681\n",
            " 840305359952719872-1255066195235143681\n",
            " 1291539303277957125-1255066195235143681\n",
            " 1288150063747944448-1255066195235143681\n",
            " 1293301280589393921-1255066195235143681\n",
            " 1289238319927308288-1255066195235143681\n",
            " 1269215111941586944-1255066195235143681\n",
            " 1373333412-1255066195235143681\n",
            " 1294878958694141956-1255066195235143681\n",
            " 1289157424088317952-1255066195235143681\n",
            " 783315967-1255066195235143681\n",
            " 1292528402809925633-1255066195235143681\n",
            " 1291953688936943618-1255066195235143681\n",
            " 1291424500752547843-1255066195235143681\n",
            " 1295332345843716097-1255066195235143681\n",
            " 930188199221170176-1255066195235143681\n",
            " 2605891879-1255066195235143681\n",
            " 1272318503320555520-1255066195235143681\n",
            " 1291418678945370118-1255066195235143681\n",
            " 1263600415645425666-1255066195235143681\n",
            " 1295406606797209601-1255066195235143681\n",
            " 163251697-1255066195235143681\n",
            " 1267744996846788608-1255066195235143681\n",
            " 1291841966234312706-1255066195235143681\n",
            " 1291014372391890948-1255066195235143681\n",
            " 1285254169000062977-1255066195235143681\n",
            " 1289556848866140161-1255066195235143681\n",
            " 1246806675564175361-1255066195235143681\n",
            " 1287779713109327872-1255066195235143681\n",
            " 1274427490166022145-1255066195235143681\n",
            " 1294839984634068993-1255066195235143681\n",
            " 1221809726008197121-1255066195235143681\n",
            " 1278744329213935616-1255066195235143681\n",
            " 1280332791234822144-1255066195235143681\n",
            " 1296211348968415233-1255066195235143681\n",
            " 1291315182329880577-1255066195235143681\n",
            " 4846475141-1255066195235143681\n",
            " 1243457146811006976-1255066195235143681\n",
            " 1295098333007159303-1255066195235143681\n",
            " 998263517500362757-1255066195235143681\n",
            " 1294384953912971266-1255066195235143681\n",
            " 1216436210983473152-1255066195235143681\n",
            " 1295432970510245891-1255066195235143681\n",
            " 367270759-1255066195235143681\n",
            " 1294278079414665218-1255066195235143681\n",
            " 219628940-1255066195235143681\n",
            " 1275247607238086656-1255066195235143681\n",
            " 1294914623041425408-1255066195235143681\n",
            " 1294228976773341184-1255066195235143681\n",
            " 1292568270520881153-1255066195235143681\n",
            " 780815146050473984-1255066195235143681\n",
            " 1240687632675409929-1255066195235143681\n",
            " 811576970303705088-1255066195235143681\n",
            " 1251905803839307777-1255066195235143681\n",
            " 343717073-1255066195235143681\n",
            " 233657084-1255066195235143681\n"
          ]
        }
      ],
      "source": [
        "g = Graph()\n",
        "\n",
        "for i in range(num_nodes):\n",
        "  g.addVertex(nodes[i])\n",
        "\n",
        "for i in range(len(edges)):\n",
        "  edge = edges.iloc[i]\n",
        "  g.addEdge(edge['Source'], edge['Target'])\n",
        "  g.addEdge(edge['Target'], edge['Source'])\n",
        "\n",
        "for v in g:\n",
        "  for w in v.getConnections():\n",
        "    print(\" %s-%s\" % (v.getId(), w.getId()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "svRAR1sIz0h0",
        "outputId": "3b2a7154-00c0-4cbe-996f-084da2fac115"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4ef2bfa6-40a0-4177-83a3-1375b716deb1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>1576229254085345281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>3029183129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>1309466216860397568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>540018030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>783315967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7909</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>2611447940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7910</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>1279183267313393665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7911</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>1251905803839307777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7912</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>2445671278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7913</th>\n",
              "      <td>1255066195235143681</td>\n",
              "      <td>1265069381974994945</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7914 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ef2bfa6-40a0-4177-83a3-1375b716deb1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ef2bfa6-40a0-4177-83a3-1375b716deb1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ef2bfa6-40a0-4177-83a3-1375b716deb1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                   Source               Target\n",
              "0     1255066195235143681  1576229254085345281\n",
              "1     1255066195235143681           3029183129\n",
              "2     1255066195235143681  1309466216860397568\n",
              "3     1255066195235143681            540018030\n",
              "4     1255066195235143681            783315967\n",
              "...                   ...                  ...\n",
              "7909  1255066195235143681           2611447940\n",
              "7910  1255066195235143681  1279183267313393665\n",
              "7911  1255066195235143681  1251905803839307777\n",
              "7912  1255066195235143681           2445671278\n",
              "7913  1255066195235143681  1265069381974994945\n",
              "\n",
              "[7914 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "dot = Digraph(comment='connection-graph', format='png', strict = True)\n",
        "\n",
        "for i in range(num_nodes):\n",
        "  dot.node(str(nodes[i]))\n",
        "\n",
        "edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mgS3T7h0FMx"
      },
      "outputs": [],
      "source": [
        "for i in range(len(edges)):\n",
        "  edge = edges.iloc[i]\n",
        "  dot.edge(str(edge['Source']), str(edge['Target']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYSDqCYd5J2v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnA4Fbm11OBS",
        "outputId": "b41237cd-e287-48cf-d5ba-07a95b7c61c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "directory:  Twitter-Bot-Detection/data/bot_user_edge/ bot:  True\n",
            "Reached file # 20\n",
            "Reached file # 40\n",
            "Reached file # 60\n",
            "Reached file # 80\n",
            "Reached file # 100\n",
            "Reached file # 120\n",
            "Reached file # 140\n",
            "Reached file # 160\n",
            "Reached file # 180\n",
            "Reached file # 200\n",
            "Reached file # 220\n",
            "Reached file # 240\n",
            "Reached file # 260\n",
            "Reached file # 280\n",
            "Reached file # 300\n",
            "Reached file # 320\n",
            "Reached file # 340\n",
            "Reached file # 360\n",
            "Reached file # 380\n",
            "Reached file # 400\n",
            "Reached file # 420\n",
            "Reached file # 440\n",
            "Reached file # 460\n",
            "Reached file # 480\n",
            "Reached file # 500\n",
            "directory:  Twitter-Bot-Detection/data/real_user_edge/ bot:  False\n",
            "Reached file # 20\n",
            "Reached file # 40\n",
            "Reached file # 60\n",
            "Reached file # 80\n",
            "Reached file # 100\n",
            "Reached file # 120\n",
            "Reached file # 140\n",
            "Reached file # 160\n",
            "Reached file # 180\n",
            "Reached file # 200\n",
            "Reached file # 220\n",
            "Reached file # 240\n",
            "Reached file # 260\n",
            "Reached file # 280\n",
            "Reached file # 300\n",
            "Reached file # 320\n",
            "Reached file # 340\n",
            "Reached file # 360\n",
            "Reached file # 380\n",
            "Reached file # 400\n",
            "Reached file # 420\n",
            "Reached file # 440\n",
            "Reached file # 460\n",
            "Reached file # 480\n",
            "Reached file # 500\n"
          ]
        }
      ],
      "source": [
        "## Graph classification\n",
        "\n",
        "graph_classification = {}\n",
        "isBot = True\n",
        "bot = 1\n",
        "real = 2\n",
        "node_labels = {}\n",
        "count = 0\n",
        "graph_number = 1\n",
        "graph_edges = {}\n",
        "\n",
        "graph_classification = {}\n",
        "\n",
        "graph_classification[bot] = []\n",
        "graph_classification[real] = []\n",
        "directories = ['Twitter-Bot-Detection/data/bot_user_edge/',\n",
        "               'Twitter-Bot-Detection/data/real_user_edge/']\n",
        "\n",
        "bot_list = []\n",
        "real_list = []\n",
        "\n",
        "def build_graph_edges(edge_list, count):\n",
        "  for edge in edge_list:\n",
        "    source = int(edge[0])\n",
        "    target = int(edge[1])\n",
        "    if source not in node_labels:\n",
        "      node_labels[source] = count\n",
        "      count += 1\n",
        "    if node_labels[source] not in graph_edges:\n",
        "      graph_edges[node_labels[source]] = set()\n",
        "    else:\n",
        "      if target not in node_labels:\n",
        "        node_labels[target] = count\n",
        "        count += 1\n",
        "      graph_edges[node_labels[source]].add(node_labels[target])\n",
        "  return count\n",
        "\n",
        "FILE_LIMIT = 500\n",
        "\n",
        "for directory in directories:\n",
        "  isBot = True if directory == 'Twitter-Bot-Detection/data/bot_user_edge/' else False\n",
        "  print('directory: ', directory, 'bot: ', isBot)\n",
        "  count_file = 0\n",
        "  for file in [file for file in os.listdir(directory) if file.endswith('.csv')]:\n",
        "    if count_file == FILE_LIMIT:\n",
        "      break\n",
        "    with open(directory + file, 'r') as csv_file:\n",
        "      edge_reader = csv.reader(csv_file)\n",
        "      edge_list = list(edge_reader)\n",
        "      graph_edges = {}\n",
        "      count = build_graph_edges(edge_list, count)\n",
        "      if isBot:\n",
        "        graph_classification[bot].append([graph_number, graph_edges])\n",
        "      else:\n",
        "        graph_classification[real].append([graph_number, graph_edges])\n",
        "      graph_number += 1\n",
        "    count_file += 1\n",
        "    if count_file%20==0: \n",
        "      print(\"Reached file #\", count_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5Sk9Dip-ryA",
        "outputId": "f9e7c03e-40cb-4387-b335-90df7d12d2c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1001\n",
            "20438856\n",
            "20438856\n"
          ]
        }
      ],
      "source": [
        "print(graph_number)\n",
        "print(count)\n",
        "print(len(node_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbiBv6zwRgH_"
      },
      "outputs": [],
      "source": [
        "import pickle as pk\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrYeindLD6fN"
      },
      "outputs": [],
      "source": [
        "with open('saved_dictionary.pkl', 'wb') as f:\n",
        "    pk.dump(graph_classification, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k47ZzjGEI51"
      },
      "outputs": [],
      "source": [
        "with open('saved_dictionary.pkl', 'rb') as f:\n",
        "    graph_classification = pk.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_yiJqAXSwFP"
      },
      "outputs": [],
      "source": [
        "b = []\n",
        "c = []\n",
        "nodes = {}\n",
        "\n",
        "for classification in graph_classification.keys():\n",
        "  for graph_list in graph_classification[classification]:\n",
        "    a = []\n",
        "    graph_edges = graph_list[1]\n",
        "    edges = set()\n",
        "    n = {}\n",
        "    labeled_edges = {}\n",
        "    for source in graph_edges.keys():\n",
        "      for target in graph_edges[source]:\n",
        "        if source not in nodes:\n",
        "          n[source] = classification\n",
        "        if target not in nodes:\n",
        "          n[target] = classification\n",
        "        edges.add((source, target))\n",
        "        labeled_edges[(source, target)] = classification\n",
        "    if len(edges) != 0:\n",
        "      a.append(edges)\n",
        "      a.append(n)\n",
        "      a.append(labeled_edges)\n",
        "      c.append(a)\n",
        "      b.append(classification)\n",
        "\n",
        "USER = {}\n",
        "USER['data'] = c\n",
        "USER['target'] = b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8QurFzL7NZj",
        "outputId": "b7e0eae3-0e57-4429-bb7e-14e3dc10dbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.7.15\n"
          ]
        }
      ],
      "source": [
        "from platform import python_version\n",
        "\n",
        "print(python_version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPDZzEJiPbrH",
        "outputId": "635ec9fd-af90-485d-a291-66a53cf72fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n",
            "<class 'dict'>\n",
            "988\n",
            "[{(0, 86), (0, 378), (0, 175), (0, 532), (0, 499), (0, 76), (0, 197), (0, 522), (0, 489), (0, 643), (0, 98), (0, 251), (0, 544), (0, 271), (0, 697), (0, 408), (0, 17), (0, 582), (0, 293), (0, 735), (0, 446), (0, 55), (0, 636), (0, 347), (0, 128), (0, 757), (0, 468), (0, 45), (0, 369), (0, 166), (0, 747), (0, 458), (0, 67), (0, 220), (0, 513), (0, 480), (0, 666), (0, 121), (0, 242), (0, 551), (0, 262), (0, 688), (0, 415), (0, 232), (0, 605), (0, 316), (0, 726), (0, 437), (0, 14), (0, 627), (0, 338), (0, 135), (0, 716), (0, 427), (0, 36), (0, 617), (0, 328), (0, 189), (0, 738), (0, 449), (0, 90), (0, 366), (0, 211), (0, 536), (0, 487), (0, 657), (0, 112), (0, 201), (0, 574), (0, 285), (0, 695), (0, 406), (0, 239), (0, 596), (0, 307), (0, 685), (0, 396), (0, 5), (0, 586), (0, 297), (0, 158), (0, 707), (0, 418), (0, 59), (0, 608), (0, 335), (0, 180), (0, 761), (0, 472), (0, 81), (0, 357), (0, 170), (0, 543), (0, 510), (0, 119), (0, 192), (0, 565), (0, 276), (0, 654), (0, 109), (0, 230), (0, 555), (0, 266), (0, 676), (0, 387), (0, 28), (0, 577), (0, 288), (0, 149), (0, 730), (0, 441), (0, 50), (0, 615), (0, 326), (0, 139), (0, 752), (0, 479), (0, 772), (0, 40), (0, 380), (0, 161), (0, 534), (0, 501), (0, 78), (0, 199), (0, 524), (0, 491), (0, 645), (0, 100), (0, 253), (0, 546), (0, 257), (0, 699), (0, 410), (0, 19), (0, 600), (0, 295), (0, 721), (0, 432), (0, 9), (0, 638), (0, 349), (0, 130), (0, 759), (0, 470), (0, 47), (0, 371), (0, 184), (0, 749), (0, 460), (0, 69), (0, 361), (0, 222), (0, 515), (0, 482), (0, 668), (0, 123), (0, 244), (0, 569), (0, 280), (0, 690), (0, 401), (0, 234), (0, 607), (0, 318), (0, 680), (0, 439), (0, 629), (0, 340), (0, 153), (0, 718), (0, 429), (0, 38), (0, 619), (0, 330), (0, 191), (0, 740), (0, 451), (0, 92), (0, 352), (0, 213), (0, 538), (0, 505), (0, 659), (0, 114), (0, 203), (0, 560), (0, 287), (0, 649), (0, 104), (0, 225), (0, 598), (0, 309), (0, 687), (0, 398), (0, 7), (0, 588), (0, 299), (0, 144), (0, 709), (0, 420), (0, 61), (0, 610), (0, 321), (0, 182), (0, 763), (0, 474), (0, 83), (0, 359), (0, 172), (0, 529), (0, 496), (0, 73), (0, 194), (0, 567), (0, 278), (0, 640), (0, 111), (0, 248), (0, 557), (0, 268), (0, 678), (0, 389), (0, 30), (0, 579), (0, 290), (0, 151), (0, 732), (0, 443), (0, 52), (0, 633), (0, 344), (0, 141), (0, 754), (0, 465), (0, 774), (0, 42), (0, 382), (0, 163), (0, 744), (0, 503), (0, 64), (0, 217), (0, 526), (0, 493), (0, 647), (0, 102), (0, 255), (0, 548), (0, 259), (0, 701), (0, 412), (0, 21), (0, 602), (0, 313), (0, 723), (0, 434), (0, 11), (0, 624), (0, 351), (0, 132), (0, 713), (0, 424), (0, 33), (0, 373), (0, 186), (0, 751), (0, 462), (0, 71), (0, 363), (0, 208), (0, 517), (0, 484), (0, 670), (0, 125), (0, 246), (0, 571), (0, 282), (0, 692), (0, 403), (0, 236), (0, 593), (0, 304), (0, 682), (0, 393), (0, 2), (0, 631), (0, 342), (0, 155), (0, 704), (0, 431), (0, 56), (0, 621), (0, 332), (0, 177), (0, 742), (0, 453), (0, 94), (0, 354), (0, 215), (0, 540), (0, 507), (0, 661), (0, 116), (0, 205), (0, 562), (0, 273), (0, 651), (0, 106), (0, 227), (0, 552), (0, 311), (0, 673), (0, 384), (0, 25), (0, 590), (0, 301), (0, 146), (0, 711), (0, 422), (0, 63), (0, 612), (0, 323), (0, 136), (0, 765), (0, 476), (0, 769), (0, 85), (0, 377), (0, 174), (0, 531), (0, 498), (0, 75), (0, 196), (0, 521), (0, 488), (0, 642), (0, 97), (0, 250), (0, 559), (0, 270), (0, 696), (0, 391), (0, 16), (0, 581), (0, 292), (0, 734), (0, 445), (0, 54), (0, 635), (0, 346), (0, 143), (0, 756), (0, 467), (0, 44), (0, 368), (0, 165), (0, 746), (0, 457), (0, 66), (0, 219), (0, 512), (0, 495), (0, 665), (0, 120), (0, 241), (0, 550), (0, 261), (0, 703), (0, 414), (0, 23), (0, 604), (0, 315), (0, 725), (0, 436), (0, 13), (0, 626), (0, 337), (0, 134), (0, 715), (0, 426), (0, 35), (0, 616), (0, 375), (0, 188), (0, 737), (0, 448), (0, 89), (0, 365), (0, 210), (0, 519), (0, 486), (0, 656), (0, 127), (0, 200), (0, 573), (0, 284), (0, 694), (0, 405), (0, 238), (0, 595), (0, 306), (0, 684), (0, 395), (0, 4), (0, 585), (0, 296), (0, 157), (0, 706), (0, 417), (0, 58), (0, 623), (0, 334), (0, 179), (0, 760), (0, 455), (0, 80), (0, 356), (0, 169), (0, 542), (0, 509), (0, 663), (0, 118), (0, 207), (0, 564), (0, 275), (0, 653), (0, 108), (0, 229), (0, 554), (0, 265), (0, 675), (0, 386), (0, 27), (0, 576), (0, 303), (0, 148), (0, 729), (0, 440), (0, 49), (0, 614), (0, 325), (0, 138), (0, 767), (0, 478), (0, 771), (0, 87), (0, 379), (0, 160), (0, 533), (0, 500), (0, 77), (0, 198), (0, 523), (0, 490), (0, 644), (0, 99), (0, 252), (0, 545), (0, 256), (0, 698), (0, 409), (0, 18), (0, 583), (0, 294), (0, 720), (0, 447), (0, 8), (0, 637), (0, 348), (0, 129), (0, 758), (0, 469), (0, 46), (0, 370), (0, 167), (0, 748), (0, 459), (0, 68), (0, 360), (0, 221), (0, 514), (0, 481), (0, 667), (0, 122), (0, 243), (0, 568), (0, 263), (0, 689), (0, 400), (0, 233), (0, 606), (0, 317), (0, 727), (0, 438), (0, 15), (0, 628), (0, 339), (0, 152), (0, 717), (0, 428), (0, 37), (0, 618), (0, 329), (0, 190), (0, 739), (0, 450), (0, 91), (0, 367), (0, 212), (0, 537), (0, 504), (0, 658), (0, 113), (0, 202), (0, 575), (0, 286), (0, 648), (0, 407), (0, 224), (0, 597), (0, 308), (0, 686), (0, 397), (0, 6), (0, 587), (0, 298), (0, 159), (0, 708), (0, 419), (0, 60), (0, 609), (0, 320), (0, 181), (0, 762), (0, 473), (0, 82), (0, 358), (0, 171), (0, 528), (0, 511), (0, 72), (0, 193), (0, 566), (0, 277), (0, 655), (0, 110), (0, 231), (0, 556), (0, 267), (0, 677), (0, 388), (0, 29), (0, 578), (0, 289), (0, 150), (0, 731), (0, 442), (0, 51), (0, 632), (0, 327), (0, 140), (0, 753), (0, 464), (0, 773), (0, 41), (0, 381), (0, 162), (0, 535), (0, 502), (0, 79), (0, 216), (0, 525), (0, 492), (0, 646), (0, 101), (0, 254), (0, 547), (0, 258), (0, 700), (0, 411), (0, 20), (0, 601), (0, 312), (0, 722), (0, 433), (0, 10), (0, 639), (0, 350), (0, 131), (0, 712), (0, 471), (0, 32), (0, 372), (0, 185), (0, 750), (0, 461), (0, 70), (0, 362), (0, 223), (0, 516), (0, 483), (0, 669), (0, 124), (0, 245), (0, 570), (0, 281), (0, 691), (0, 402), (0, 235), (0, 592), (0, 319), (0, 681), (0, 392), (0, 1), (0, 630), (0, 341), (0, 154), (0, 719), (0, 430), (0, 39), (0, 620), (0, 331), (0, 176), (0, 741), (0, 452), (0, 93), (0, 353), (0, 214), (0, 539), (0, 506), (0, 660), (0, 115), (0, 204), (0, 561), (0, 272), (0, 650), (0, 105), (0, 226), (0, 599), (0, 310), (0, 672), (0, 399), (0, 24), (0, 589), (0, 300), (0, 145), (0, 710), (0, 421), (0, 62), (0, 611), (0, 322), (0, 183), (0, 764), (0, 475), (0, 768), (0, 84), (0, 376), (0, 173), (0, 530), (0, 497), (0, 74), (0, 195), (0, 520), (0, 279), (0, 641), (0, 96), (0, 249), (0, 558), (0, 269), (0, 679), (0, 390), (0, 31), (0, 580), (0, 291), (0, 733), (0, 444), (0, 53), (0, 634), (0, 345), (0, 142), (0, 755), (0, 466), (0, 43), (0, 383), (0, 164), (0, 745), (0, 456), (0, 65), (0, 218), (0, 527), (0, 494), (0, 664), (0, 103), (0, 240), (0, 549), (0, 260), (0, 702), (0, 413), (0, 22), (0, 603), (0, 314), (0, 724), (0, 435), (0, 12), (0, 625), (0, 336), (0, 133), (0, 714), (0, 425), (0, 34), (0, 374), (0, 187), (0, 736), (0, 463), (0, 88), (0, 364), (0, 209), (0, 518), (0, 485), (0, 671), (0, 126), (0, 247), (0, 572), (0, 283), (0, 693), (0, 404), (0, 237), (0, 594), (0, 305), (0, 683), (0, 394), (0, 3), (0, 584), (0, 343), (0, 156), (0, 705), (0, 416), (0, 57), (0, 622), (0, 333), (0, 178), (0, 743), (0, 454), (0, 95), (0, 355), (0, 168), (0, 541), (0, 508), (0, 662), (0, 117), (0, 206), (0, 563), (0, 274), (0, 652), (0, 107), (0, 228), (0, 553), (0, 264), (0, 674), (0, 385), (0, 26), (0, 591), (0, 302), (0, 147), (0, 728), (0, 423), (0, 48), (0, 613), (0, 324), (0, 137), (0, 766), (0, 477), (0, 770)}\n",
            " {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 45: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1, 53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 1, 63: 1, 64: 1, 65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 1, 94: 1, 95: 1, 96: 1, 97: 1, 98: 1, 99: 1, 100: 1, 101: 1, 102: 1, 103: 1, 104: 1, 105: 1, 106: 1, 107: 1, 108: 1, 109: 1, 110: 1, 111: 1, 112: 1, 113: 1, 114: 1, 115: 1, 116: 1, 117: 1, 118: 1, 119: 1, 120: 1, 121: 1, 122: 1, 123: 1, 124: 1, 125: 1, 126: 1, 127: 1, 128: 1, 129: 1, 130: 1, 131: 1, 132: 1, 133: 1, 134: 1, 135: 1, 136: 1, 137: 1, 138: 1, 139: 1, 140: 1, 141: 1, 142: 1, 143: 1, 144: 1, 145: 1, 146: 1, 147: 1, 148: 1, 149: 1, 150: 1, 151: 1, 152: 1, 153: 1, 154: 1, 155: 1, 156: 1, 157: 1, 158: 1, 159: 1, 160: 1, 161: 1, 162: 1, 163: 1, 164: 1, 165: 1, 166: 1, 167: 1, 168: 1, 169: 1, 170: 1, 171: 1, 172: 1, 173: 1, 174: 1, 175: 1, 176: 1, 177: 1, 178: 1, 179: 1, 180: 1, 181: 1, 182: 1, 183: 1, 184: 1, 185: 1, 186: 1, 187: 1, 188: 1, 189: 1, 190: 1, 191: 1, 192: 1, 193: 1, 194: 1, 195: 1, 196: 1, 197: 1, 198: 1, 199: 1, 200: 1, 201: 1, 202: 1, 203: 1, 204: 1, 205: 1, 206: 1, 207: 1, 208: 1, 209: 1, 210: 1, 211: 1, 212: 1, 213: 1, 214: 1, 215: 1, 216: 1, 217: 1, 218: 1, 219: 1, 220: 1, 221: 1, 222: 1, 223: 1, 224: 1, 225: 1, 226: 1, 227: 1, 228: 1, 229: 1, 230: 1, 231: 1, 232: 1, 233: 1, 234: 1, 235: 1, 236: 1, 237: 1, 238: 1, 239: 1, 240: 1, 241: 1, 242: 1, 243: 1, 244: 1, 245: 1, 246: 1, 247: 1, 248: 1, 249: 1, 250: 1, 251: 1, 252: 1, 253: 1, 254: 1, 255: 1, 256: 1, 257: 1, 258: 1, 259: 1, 260: 1, 261: 1, 262: 1, 263: 1, 264: 1, 265: 1, 266: 1, 267: 1, 268: 1, 269: 1, 270: 1, 271: 1, 272: 1, 273: 1, 274: 1, 275: 1, 276: 1, 277: 1, 278: 1, 279: 1, 280: 1, 281: 1, 282: 1, 283: 1, 284: 1, 285: 1, 286: 1, 287: 1, 288: 1, 289: 1, 290: 1, 291: 1, 292: 1, 293: 1, 294: 1, 295: 1, 296: 1, 297: 1, 298: 1, 299: 1, 300: 1, 301: 1, 302: 1, 303: 1, 304: 1, 305: 1, 306: 1, 307: 1, 308: 1, 309: 1, 310: 1, 311: 1, 312: 1, 313: 1, 314: 1, 315: 1, 316: 1, 317: 1, 318: 1, 319: 1, 320: 1, 321: 1, 322: 1, 323: 1, 324: 1, 325: 1, 326: 1, 327: 1, 328: 1, 329: 1, 330: 1, 331: 1, 332: 1, 333: 1, 334: 1, 335: 1, 336: 1, 337: 1, 338: 1, 339: 1, 340: 1, 341: 1, 342: 1, 343: 1, 344: 1, 345: 1, 346: 1, 347: 1, 348: 1, 349: 1, 350: 1, 351: 1, 352: 1, 353: 1, 354: 1, 355: 1, 356: 1, 357: 1, 358: 1, 359: 1, 360: 1, 361: 1, 362: 1, 363: 1, 364: 1, 365: 1, 366: 1, 367: 1, 368: 1, 369: 1, 370: 1, 371: 1, 372: 1, 373: 1, 374: 1, 375: 1, 376: 1, 377: 1, 378: 1, 379: 1, 380: 1, 381: 1, 382: 1, 383: 1, 384: 1, 385: 1, 386: 1, 387: 1, 388: 1, 389: 1, 390: 1, 391: 1, 392: 1, 393: 1, 394: 1, 395: 1, 396: 1, 397: 1, 398: 1, 399: 1, 400: 1, 401: 1, 402: 1, 403: 1, 404: 1, 405: 1, 406: 1, 407: 1, 408: 1, 409: 1, 410: 1, 411: 1, 412: 1, 413: 1, 414: 1, 415: 1, 416: 1, 417: 1, 418: 1, 419: 1, 420: 1, 421: 1, 422: 1, 423: 1, 424: 1, 425: 1, 426: 1, 427: 1, 428: 1, 429: 1, 430: 1, 431: 1, 432: 1, 433: 1, 434: 1, 435: 1, 436: 1, 437: 1, 438: 1, 439: 1, 440: 1, 441: 1, 442: 1, 443: 1, 444: 1, 445: 1, 446: 1, 447: 1, 448: 1, 449: 1, 450: 1, 451: 1, 452: 1, 453: 1, 454: 1, 455: 1, 456: 1, 457: 1, 458: 1, 459: 1, 460: 1, 461: 1, 462: 1, 463: 1, 464: 1, 465: 1, 466: 1, 467: 1, 468: 1, 469: 1, 470: 1, 471: 1, 472: 1, 473: 1, 474: 1, 475: 1, 476: 1, 477: 1, 478: 1, 479: 1, 480: 1, 481: 1, 482: 1, 483: 1, 484: 1, 485: 1, 486: 1, 487: 1, 488: 1, 489: 1, 490: 1, 491: 1, 492: 1, 493: 1, 494: 1, 495: 1, 496: 1, 497: 1, 498: 1, 499: 1, 500: 1, 501: 1, 502: 1, 503: 1, 504: 1, 505: 1, 506: 1, 507: 1, 508: 1, 509: 1, 510: 1, 511: 1, 512: 1, 513: 1, 514: 1, 515: 1, 516: 1, 517: 1, 518: 1, 519: 1, 520: 1, 521: 1, 522: 1, 523: 1, 524: 1, 525: 1, 526: 1, 527: 1, 528: 1, 529: 1, 530: 1, 531: 1, 532: 1, 533: 1, 534: 1, 535: 1, 536: 1, 537: 1, 538: 1, 539: 1, 540: 1, 541: 1, 542: 1, 543: 1, 544: 1, 545: 1, 546: 1, 547: 1, 548: 1, 549: 1, 550: 1, 551: 1, 552: 1, 553: 1, 554: 1, 555: 1, 556: 1, 557: 1, 558: 1, 559: 1, 560: 1, 561: 1, 562: 1, 563: 1, 564: 1, 565: 1, 566: 1, 567: 1, 568: 1, 569: 1, 570: 1, 571: 1, 572: 1, 573: 1, 574: 1, 575: 1, 576: 1, 577: 1, 578: 1, 579: 1, 580: 1, 581: 1, 582: 1, 583: 1, 584: 1, 585: 1, 586: 1, 587: 1, 588: 1, 589: 1, 590: 1, 591: 1, 592: 1, 593: 1, 594: 1, 595: 1, 596: 1, 597: 1, 598: 1, 599: 1, 600: 1, 601: 1, 602: 1, 603: 1, 604: 1, 605: 1, 606: 1, 607: 1, 608: 1, 609: 1, 610: 1, 611: 1, 612: 1, 613: 1, 614: 1, 615: 1, 616: 1, 617: 1, 618: 1, 619: 1, 620: 1, 621: 1, 622: 1, 623: 1, 624: 1, 625: 1, 626: 1, 627: 1, 628: 1, 629: 1, 630: 1, 631: 1, 632: 1, 633: 1, 634: 1, 635: 1, 636: 1, 637: 1, 638: 1, 639: 1, 640: 1, 641: 1, 642: 1, 643: 1, 644: 1, 645: 1, 646: 1, 647: 1, 648: 1, 649: 1, 650: 1, 651: 1, 652: 1, 653: 1, 654: 1, 655: 1, 656: 1, 657: 1, 658: 1, 659: 1, 660: 1, 661: 1, 662: 1, 663: 1, 664: 1, 665: 1, 666: 1, 667: 1, 668: 1, 669: 1, 670: 1, 671: 1, 672: 1, 673: 1, 674: 1, 675: 1, 676: 1, 677: 1, 678: 1, 679: 1, 680: 1, 681: 1, 682: 1, 683: 1, 684: 1, 685: 1, 686: 1, 687: 1, 688: 1, 689: 1, 690: 1, 691: 1, 692: 1, 693: 1, 694: 1, 695: 1, 696: 1, 697: 1, 698: 1, 699: 1, 700: 1, 701: 1, 702: 1, 703: 1, 704: 1, 705: 1, 706: 1, 707: 1, 708: 1, 709: 1, 710: 1, 711: 1, 712: 1, 713: 1, 714: 1, 715: 1, 716: 1, 717: 1, 718: 1, 719: 1, 720: 1, 721: 1, 722: 1, 723: 1, 724: 1, 725: 1, 726: 1, 727: 1, 728: 1, 729: 1, 730: 1, 731: 1, 732: 1, 733: 1, 734: 1, 735: 1, 736: 1, 737: 1, 738: 1, 739: 1, 740: 1, 741: 1, 742: 1, 743: 1, 744: 1, 745: 1, 746: 1, 747: 1, 748: 1, 749: 1, 750: 1, 751: 1, 752: 1, 753: 1, 754: 1, 755: 1, 756: 1, 757: 1, 758: 1, 759: 1, 760: 1, 761: 1, 762: 1, 763: 1, 764: 1, 765: 1, 766: 1, 767: 1, 768: 1, 769: 1, 770: 1, 771: 1, 772: 1, 773: 1, 774: 1}\n",
            " {(0, 1): 1, (0, 2): 1, (0, 3): 1, (0, 4): 1, (0, 5): 1, (0, 6): 1, (0, 7): 1, (0, 8): 1, (0, 9): 1, (0, 10): 1, (0, 11): 1, (0, 12): 1, (0, 13): 1, (0, 14): 1, (0, 15): 1, (0, 16): 1, (0, 17): 1, (0, 18): 1, (0, 19): 1, (0, 20): 1, (0, 21): 1, (0, 22): 1, (0, 23): 1, (0, 24): 1, (0, 25): 1, (0, 26): 1, (0, 27): 1, (0, 28): 1, (0, 29): 1, (0, 30): 1, (0, 31): 1, (0, 32): 1, (0, 33): 1, (0, 34): 1, (0, 35): 1, (0, 36): 1, (0, 37): 1, (0, 38): 1, (0, 39): 1, (0, 40): 1, (0, 41): 1, (0, 42): 1, (0, 43): 1, (0, 44): 1, (0, 45): 1, (0, 46): 1, (0, 47): 1, (0, 48): 1, (0, 49): 1, (0, 50): 1, (0, 51): 1, (0, 52): 1, (0, 53): 1, (0, 54): 1, (0, 55): 1, (0, 56): 1, (0, 57): 1, (0, 58): 1, (0, 59): 1, (0, 60): 1, (0, 61): 1, (0, 62): 1, (0, 63): 1, (0, 64): 1, (0, 65): 1, (0, 66): 1, (0, 67): 1, (0, 68): 1, (0, 69): 1, (0, 70): 1, (0, 71): 1, (0, 72): 1, (0, 73): 1, (0, 74): 1, (0, 75): 1, (0, 76): 1, (0, 77): 1, (0, 78): 1, (0, 79): 1, (0, 80): 1, (0, 81): 1, (0, 82): 1, (0, 83): 1, (0, 84): 1, (0, 85): 1, (0, 86): 1, (0, 87): 1, (0, 88): 1, (0, 89): 1, (0, 90): 1, (0, 91): 1, (0, 92): 1, (0, 93): 1, (0, 94): 1, (0, 95): 1, (0, 96): 1, (0, 97): 1, (0, 98): 1, (0, 99): 1, (0, 100): 1, (0, 101): 1, (0, 102): 1, (0, 103): 1, (0, 104): 1, (0, 105): 1, (0, 106): 1, (0, 107): 1, (0, 108): 1, (0, 109): 1, (0, 110): 1, (0, 111): 1, (0, 112): 1, (0, 113): 1, (0, 114): 1, (0, 115): 1, (0, 116): 1, (0, 117): 1, (0, 118): 1, (0, 119): 1, (0, 120): 1, (0, 121): 1, (0, 122): 1, (0, 123): 1, (0, 124): 1, (0, 125): 1, (0, 126): 1, (0, 127): 1, (0, 128): 1, (0, 129): 1, (0, 130): 1, (0, 131): 1, (0, 132): 1, (0, 133): 1, (0, 134): 1, (0, 135): 1, (0, 136): 1, (0, 137): 1, (0, 138): 1, (0, 139): 1, (0, 140): 1, (0, 141): 1, (0, 142): 1, (0, 143): 1, (0, 144): 1, (0, 145): 1, (0, 146): 1, (0, 147): 1, (0, 148): 1, (0, 149): 1, (0, 150): 1, (0, 151): 1, (0, 152): 1, (0, 153): 1, (0, 154): 1, (0, 155): 1, (0, 156): 1, (0, 157): 1, (0, 158): 1, (0, 159): 1, (0, 160): 1, (0, 161): 1, (0, 162): 1, (0, 163): 1, (0, 164): 1, (0, 165): 1, (0, 166): 1, (0, 167): 1, (0, 168): 1, (0, 169): 1, (0, 170): 1, (0, 171): 1, (0, 172): 1, (0, 173): 1, (0, 174): 1, (0, 175): 1, (0, 176): 1, (0, 177): 1, (0, 178): 1, (0, 179): 1, (0, 180): 1, (0, 181): 1, (0, 182): 1, (0, 183): 1, (0, 184): 1, (0, 185): 1, (0, 186): 1, (0, 187): 1, (0, 188): 1, (0, 189): 1, (0, 190): 1, (0, 191): 1, (0, 192): 1, (0, 193): 1, (0, 194): 1, (0, 195): 1, (0, 196): 1, (0, 197): 1, (0, 198): 1, (0, 199): 1, (0, 200): 1, (0, 201): 1, (0, 202): 1, (0, 203): 1, (0, 204): 1, (0, 205): 1, (0, 206): 1, (0, 207): 1, (0, 208): 1, (0, 209): 1, (0, 210): 1, (0, 211): 1, (0, 212): 1, (0, 213): 1, (0, 214): 1, (0, 215): 1, (0, 216): 1, (0, 217): 1, (0, 218): 1, (0, 219): 1, (0, 220): 1, (0, 221): 1, (0, 222): 1, (0, 223): 1, (0, 224): 1, (0, 225): 1, (0, 226): 1, (0, 227): 1, (0, 228): 1, (0, 229): 1, (0, 230): 1, (0, 231): 1, (0, 232): 1, (0, 233): 1, (0, 234): 1, (0, 235): 1, (0, 236): 1, (0, 237): 1, (0, 238): 1, (0, 239): 1, (0, 240): 1, (0, 241): 1, (0, 242): 1, (0, 243): 1, (0, 244): 1, (0, 245): 1, (0, 246): 1, (0, 247): 1, (0, 248): 1, (0, 249): 1, (0, 250): 1, (0, 251): 1, (0, 252): 1, (0, 253): 1, (0, 254): 1, (0, 255): 1, (0, 256): 1, (0, 257): 1, (0, 258): 1, (0, 259): 1, (0, 260): 1, (0, 261): 1, (0, 262): 1, (0, 263): 1, (0, 264): 1, (0, 265): 1, (0, 266): 1, (0, 267): 1, (0, 268): 1, (0, 269): 1, (0, 270): 1, (0, 271): 1, (0, 272): 1, (0, 273): 1, (0, 274): 1, (0, 275): 1, (0, 276): 1, (0, 277): 1, (0, 278): 1, (0, 279): 1, (0, 280): 1, (0, 281): 1, (0, 282): 1, (0, 283): 1, (0, 284): 1, (0, 285): 1, (0, 286): 1, (0, 287): 1, (0, 288): 1, (0, 289): 1, (0, 290): 1, (0, 291): 1, (0, 292): 1, (0, 293): 1, (0, 294): 1, (0, 295): 1, (0, 296): 1, (0, 297): 1, (0, 298): 1, (0, 299): 1, (0, 300): 1, (0, 301): 1, (0, 302): 1, (0, 303): 1, (0, 304): 1, (0, 305): 1, (0, 306): 1, (0, 307): 1, (0, 308): 1, (0, 309): 1, (0, 310): 1, (0, 311): 1, (0, 312): 1, (0, 313): 1, (0, 314): 1, (0, 315): 1, (0, 316): 1, (0, 317): 1, (0, 318): 1, (0, 319): 1, (0, 320): 1, (0, 321): 1, (0, 322): 1, (0, 323): 1, (0, 324): 1, (0, 325): 1, (0, 326): 1, (0, 327): 1, (0, 328): 1, (0, 329): 1, (0, 330): 1, (0, 331): 1, (0, 332): 1, (0, 333): 1, (0, 334): 1, (0, 335): 1, (0, 336): 1, (0, 337): 1, (0, 338): 1, (0, 339): 1, (0, 340): 1, (0, 341): 1, (0, 342): 1, (0, 343): 1, (0, 344): 1, (0, 345): 1, (0, 346): 1, (0, 347): 1, (0, 348): 1, (0, 349): 1, (0, 350): 1, (0, 351): 1, (0, 352): 1, (0, 353): 1, (0, 354): 1, (0, 355): 1, (0, 356): 1, (0, 357): 1, (0, 358): 1, (0, 359): 1, (0, 360): 1, (0, 361): 1, (0, 362): 1, (0, 363): 1, (0, 364): 1, (0, 365): 1, (0, 366): 1, (0, 367): 1, (0, 368): 1, (0, 369): 1, (0, 370): 1, (0, 371): 1, (0, 372): 1, (0, 373): 1, (0, 374): 1, (0, 375): 1, (0, 376): 1, (0, 377): 1, (0, 378): 1, (0, 379): 1, (0, 380): 1, (0, 381): 1, (0, 382): 1, (0, 383): 1, (0, 384): 1, (0, 385): 1, (0, 386): 1, (0, 387): 1, (0, 388): 1, (0, 389): 1, (0, 390): 1, (0, 391): 1, (0, 392): 1, (0, 393): 1, (0, 394): 1, (0, 395): 1, (0, 396): 1, (0, 397): 1, (0, 398): 1, (0, 399): 1, (0, 400): 1, (0, 401): 1, (0, 402): 1, (0, 403): 1, (0, 404): 1, (0, 405): 1, (0, 406): 1, (0, 407): 1, (0, 408): 1, (0, 409): 1, (0, 410): 1, (0, 411): 1, (0, 412): 1, (0, 413): 1, (0, 414): 1, (0, 415): 1, (0, 416): 1, (0, 417): 1, (0, 418): 1, (0, 419): 1, (0, 420): 1, (0, 421): 1, (0, 422): 1, (0, 423): 1, (0, 424): 1, (0, 425): 1, (0, 426): 1, (0, 427): 1, (0, 428): 1, (0, 429): 1, (0, 430): 1, (0, 431): 1, (0, 432): 1, (0, 433): 1, (0, 434): 1, (0, 435): 1, (0, 436): 1, (0, 437): 1, (0, 438): 1, (0, 439): 1, (0, 440): 1, (0, 441): 1, (0, 442): 1, (0, 443): 1, (0, 444): 1, (0, 445): 1, (0, 446): 1, (0, 447): 1, (0, 448): 1, (0, 449): 1, (0, 450): 1, (0, 451): 1, (0, 452): 1, (0, 453): 1, (0, 454): 1, (0, 455): 1, (0, 456): 1, (0, 457): 1, (0, 458): 1, (0, 459): 1, (0, 460): 1, (0, 461): 1, (0, 462): 1, (0, 463): 1, (0, 464): 1, (0, 465): 1, (0, 466): 1, (0, 467): 1, (0, 468): 1, (0, 469): 1, (0, 470): 1, (0, 471): 1, (0, 472): 1, (0, 473): 1, (0, 474): 1, (0, 475): 1, (0, 476): 1, (0, 477): 1, (0, 478): 1, (0, 479): 1, (0, 480): 1, (0, 481): 1, (0, 482): 1, (0, 483): 1, (0, 484): 1, (0, 485): 1, (0, 486): 1, (0, 487): 1, (0, 488): 1, (0, 489): 1, (0, 490): 1, (0, 491): 1, (0, 492): 1, (0, 493): 1, (0, 494): 1, (0, 495): 1, (0, 496): 1, (0, 497): 1, (0, 498): 1, (0, 499): 1, (0, 500): 1, (0, 501): 1, (0, 502): 1, (0, 503): 1, (0, 504): 1, (0, 505): 1, (0, 506): 1, (0, 507): 1, (0, 508): 1, (0, 509): 1, (0, 510): 1, (0, 511): 1, (0, 512): 1, (0, 513): 1, (0, 514): 1, (0, 515): 1, (0, 516): 1, (0, 517): 1, (0, 518): 1, (0, 519): 1, (0, 520): 1, (0, 521): 1, (0, 522): 1, (0, 523): 1, (0, 524): 1, (0, 525): 1, (0, 526): 1, (0, 527): 1, (0, 528): 1, (0, 529): 1, (0, 530): 1, (0, 531): 1, (0, 532): 1, (0, 533): 1, (0, 534): 1, (0, 535): 1, (0, 536): 1, (0, 537): 1, (0, 538): 1, (0, 539): 1, (0, 540): 1, (0, 541): 1, (0, 542): 1, (0, 543): 1, (0, 544): 1, (0, 545): 1, (0, 546): 1, (0, 547): 1, (0, 548): 1, (0, 549): 1, (0, 550): 1, (0, 551): 1, (0, 552): 1, (0, 553): 1, (0, 554): 1, (0, 555): 1, (0, 556): 1, (0, 557): 1, (0, 558): 1, (0, 559): 1, (0, 560): 1, (0, 561): 1, (0, 562): 1, (0, 563): 1, (0, 564): 1, (0, 565): 1, (0, 566): 1, (0, 567): 1, (0, 568): 1, (0, 569): 1, (0, 570): 1, (0, 571): 1, (0, 572): 1, (0, 573): 1, (0, 574): 1, (0, 575): 1, (0, 576): 1, (0, 577): 1, (0, 578): 1, (0, 579): 1, (0, 580): 1, (0, 581): 1, (0, 582): 1, (0, 583): 1, (0, 584): 1, (0, 585): 1, (0, 586): 1, (0, 587): 1, (0, 588): 1, (0, 589): 1, (0, 590): 1, (0, 591): 1, (0, 592): 1, (0, 593): 1, (0, 594): 1, (0, 595): 1, (0, 596): 1, (0, 597): 1, (0, 598): 1, (0, 599): 1, (0, 600): 1, (0, 601): 1, (0, 602): 1, (0, 603): 1, (0, 604): 1, (0, 605): 1, (0, 606): 1, (0, 607): 1, (0, 608): 1, (0, 609): 1, (0, 610): 1, (0, 611): 1, (0, 612): 1, (0, 613): 1, (0, 614): 1, (0, 615): 1, (0, 616): 1, (0, 617): 1, (0, 618): 1, (0, 619): 1, (0, 620): 1, (0, 621): 1, (0, 622): 1, (0, 623): 1, (0, 624): 1, (0, 625): 1, (0, 626): 1, (0, 627): 1, (0, 628): 1, (0, 629): 1, (0, 630): 1, (0, 631): 1, (0, 632): 1, (0, 633): 1, (0, 634): 1, (0, 635): 1, (0, 636): 1, (0, 637): 1, (0, 638): 1, (0, 639): 1, (0, 640): 1, (0, 641): 1, (0, 642): 1, (0, 643): 1, (0, 644): 1, (0, 645): 1, (0, 646): 1, (0, 647): 1, (0, 648): 1, (0, 649): 1, (0, 650): 1, (0, 651): 1, (0, 652): 1, (0, 653): 1, (0, 654): 1, (0, 655): 1, (0, 656): 1, (0, 657): 1, (0, 658): 1, (0, 659): 1, (0, 660): 1, (0, 661): 1, (0, 662): 1, (0, 663): 1, (0, 664): 1, (0, 665): 1, (0, 666): 1, (0, 667): 1, (0, 668): 1, (0, 669): 1, (0, 670): 1, (0, 671): 1, (0, 672): 1, (0, 673): 1, (0, 674): 1, (0, 675): 1, (0, 676): 1, (0, 677): 1, (0, 678): 1, (0, 679): 1, (0, 680): 1, (0, 681): 1, (0, 682): 1, (0, 683): 1, (0, 684): 1, (0, 685): 1, (0, 686): 1, (0, 687): 1, (0, 688): 1, (0, 689): 1, (0, 690): 1, (0, 691): 1, (0, 692): 1, (0, 693): 1, (0, 694): 1, (0, 695): 1, (0, 696): 1, (0, 697): 1, (0, 698): 1, (0, 699): 1, (0, 700): 1, (0, 701): 1, (0, 702): 1, (0, 703): 1, (0, 704): 1, (0, 705): 1, (0, 706): 1, (0, 707): 1, (0, 708): 1, (0, 709): 1, (0, 710): 1, (0, 711): 1, (0, 712): 1, (0, 713): 1, (0, 714): 1, (0, 715): 1, (0, 716): 1, (0, 717): 1, (0, 718): 1, (0, 719): 1, (0, 720): 1, (0, 721): 1, (0, 722): 1, (0, 723): 1, (0, 724): 1, (0, 725): 1, (0, 726): 1, (0, 727): 1, (0, 728): 1, (0, 729): 1, (0, 730): 1, (0, 731): 1, (0, 732): 1, (0, 733): 1, (0, 734): 1, (0, 735): 1, (0, 736): 1, (0, 737): 1, (0, 738): 1, (0, 739): 1, (0, 740): 1, (0, 741): 1, (0, 742): 1, (0, 743): 1, (0, 744): 1, (0, 745): 1, (0, 746): 1, (0, 747): 1, (0, 748): 1, (0, 749): 1, (0, 750): 1, (0, 751): 1, (0, 752): 1, (0, 753): 1, (0, 754): 1, (0, 755): 1, (0, 756): 1, (0, 757): 1, (0, 758): 1, (0, 759): 1, (0, 760): 1, (0, 761): 1, (0, 762): 1, (0, 763): 1, (0, 764): 1, (0, 765): 1, (0, 766): 1, (0, 767): 1, (0, 768): 1, (0, 769): 1, (0, 770): 1, (0, 771): 1, (0, 772): 1, (0, 773): 1, (0, 774): 1}]\n",
            "988\n"
          ]
        }
      ],
      "source": [
        "G = USER['data']\n",
        "y = USER['target']\n",
        "G = np.asarray(G)\n",
        "y = np.asarray(y)\n",
        "print(G.ndim)\n",
        "print(y.ndim)\n",
        "print(type(G[1][1]))\n",
        "print(len(G))\n",
        "print(G[0])\n",
        "print(len(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTYoJBV-Vgk2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9t5zFEWPrJ_",
        "outputId": "b9452f69-ae03-4652-f3ea-c2d4b8d9d726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: grakel in /usr/local/lib/python3.7/dist-packages (0.1.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from grakel) (1.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from grakel) (1.15.0)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from grakel) (0.16.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from grakel) (0.29.32)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from grakel) (1.21.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.7/dist-packages (from grakel) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19->grakel) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19->grakel) (1.7.3)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the WL subtree kernel framework\n",
        "\n",
        "!pip install grakel\n",
        "from grakel.kernels import WeisfeilerLehman, VertexHistogram\n",
        "wl_kernel = WeisfeilerLehman(n_iter=10, normalize=False, base_graph_kernel=VertexHistogram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMQgiLi4PvYD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "X=G\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "K_train = wl_kernel.fit_transform(X_train)\n",
        "K_test = wl_kernel.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_M68s_-GFbV",
        "outputId": "0ea29d99-e127-46dd-f188-55e3dbe72168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results for  2  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.24 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.56 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "\n",
            "Results for  2  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.24 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.56 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  3  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.94 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "\n",
            "Results for  2  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.24 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.56 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  3  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.94 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  4  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.19 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "\n",
            "Results for  2  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.24 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.56 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  3  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.94 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  4  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.19 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  5  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.57 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "\n",
            "Results for  2  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.24 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.56 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  3  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.94 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  4  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.19 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  5  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.57 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  6  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.70 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "\n",
            "Results for  2  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.24 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.56 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  3  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.94 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  4  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.19 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  5  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.57 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  6  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.70 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  7  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.94 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "\n",
            "Results for  2  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.24 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.56 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  3  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.94 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  4  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.19 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  5  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.57 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  6  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.70 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  7  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.94 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  8  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 86.20 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "\n",
            "Results for  2  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.24 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.56 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  3  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 84.94 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  4  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.19 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  5  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.57 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  6  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.70 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  7  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 85.94 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  8  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 86.20 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "Results for  9  fold cross-validation\n",
            "Accuracy of  AdaBoost  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Multi Layer Perceptron  with Cross Validation is: 100.00 %\n",
            "Accuracy of  K Nearest Neighbors  with Cross Validation is: 99.75 %\n",
            "Accuracy of  Gaussian Naive Bayes  with Cross Validation is: 98.73 %\n",
            "Accuracy of  Decision Tree  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Random Forest  with Cross Validation is: 100.00 %\n",
            "Accuracy of  Support Vector Classification  with Cross Validation is: 86.07 %\n",
            "Accuracy of  Logistic Regression  with Cross Validation is: 100.00 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "models = []\n",
        "models.append((\"AdaBoost\",AdaBoostClassifier()))\n",
        "models.append((\"Multi Layer Perceptron\",MLPClassifier()))\n",
        "models.append((\"K Nearest Neighbors\",KNeighborsClassifier()))\n",
        "models.append((\"Gaussian Naive Bayes\",GaussianNB()))\n",
        "models.append((\"Decision Tree\",DecisionTreeClassifier()))\n",
        "models.append((\"Random Forest\",RandomForestClassifier()))\n",
        "models.append((\"Support Vector Classification\",SVC()))\n",
        "models.append((\"Logistic Regression\",LogisticRegression()))\n",
        "\n",
        "\n",
        "for i in range(2,11):\n",
        "  for n in range(2, i):\n",
        "    print('Results for ', n ,' fold cross-validation')\n",
        "    for name,model in models:\n",
        "      kfold = KFold(n_splits=n, random_state=42, shuffle=True)\n",
        "    \n",
        "      model.fit(K_train, y_train)\n",
        "      cv_result = cross_val_score(model,K_train,y_train, cv = kfold, scoring = \"accuracy\")\n",
        "      print(\"Accuracy of \", name,\" with Cross Validation is: {:.2f} %\".format(cv_result.mean()*100))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjzjI6R0jSVn"
      },
      "outputs": [],
      "source": [
        "# Split dataset into trainig and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from grakel.kernels import WeisfeilerLehman, VertexHistogram\n",
        "\n",
        "# Matrices to store train and test dataset\n",
        "def wl_kernel_fit_transform(G, y):\n",
        "  G_train, G_test, y_train, y_test = train_test_split(G, y, random_state=42, shuffle=True)\n",
        "  print(len(G_train))\n",
        "  print(len(y_train))\n",
        "  wl_kernel = WeisfeilerLehman(n_iter=10, normalize=False, base_graph_kernel=VertexHistogram)\n",
        "  K_train = wl_kernel.fit_transform(G_train)\n",
        "  K_test = wl_kernel.transform(G_test)\n",
        "  answer = [K_train, K_test, y_train, y_test]\n",
        "  print(type(answer))\n",
        "  print(answer)\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy99mN5mjVvR",
        "outputId": "cf9b4b4c-ba5c-4012-9870-b6aec65bc41a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "741\n",
            "741\n",
            "<class 'list'>\n",
            "[array([[    203739,          0,          0, ...,     384866,          0,\n",
            "                 0],\n",
            "       [         0,   64585076,  139407692, ...,          0,  795064600,\n",
            "          14669396],\n",
            "       [         0,  139407692,  300913319, ...,          0, 1716157120,\n",
            "          31664082],\n",
            "       ...,\n",
            "       [    384866,          0,          0, ...,     727064,          0,\n",
            "                 0],\n",
            "       [         0,  795064600, 1716157120, ...,          0, 9787521566,\n",
            "         180585360],\n",
            "       [         0,   14669396,   31664082, ...,          0,  180585360,\n",
            "           3331976]]), array([[          0,  1241771116,  2680378862, ...,           0,\n",
            "        15286633852,   282047156],\n",
            "       [          0,  1145523532,  2472627229, ...,           0,\n",
            "        14101792652,   260186212],\n",
            "       [  118473569,           0,           0, ...,   223809756,\n",
            "                  0,           0],\n",
            "       ...,\n",
            "       [   23446973,           0,           0, ...,    44293942,\n",
            "                  0,           0],\n",
            "       [          0,     8238510,    17782930, ...,           0,\n",
            "          101418920,     1871240],\n",
            "       [   22633016,           0,           0, ...,    42756284,\n",
            "                  0,           0]]), array([2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
            "       1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
            "       1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
            "       1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
            "       1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
            "       2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
            "       2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2,\n",
            "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "       2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2,\n",
            "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1]), array([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
            "       2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
            "       2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "       1, 1, 2, 1, 2])]\n",
            "ACC: 1.0\n",
            "ROC: 1.0\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
        "\n",
        "k_matrices = wl_kernel_fit_transform(G,y)\n",
        "clf = SVC(kernel='precomputed')\n",
        "clf.fit(k_matrices[0], k_matrices[2])\n",
        "\n",
        "y_prediction = clf.predict(k_matrices[1])\n",
        "\n",
        "# print the classification accuracy\n",
        "from sklearn import metrics\n",
        "\n",
        "accuracies = metrics.accuracy_score(k_matrices[3], y_prediction)\n",
        "\n",
        "print(\"ACC: {}\".format(accuracy_score(k_matrices[3], y_prediction)))\n",
        "print(\"ROC: {}\".format(roc_auc_score(k_matrices[3], y_prediction)))\n",
        "print(\"F1: {}\".format(f1_score(k_matrices[3], y_prediction)))\n",
        "print(\"Precision: {}\".format(precision_score(k_matrices[3], y_prediction)))\n",
        "print(\"Recall: {}\\n\".format(recall_score(k_matrices[3], y_prediction)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgwoQqUYjYS0",
        "outputId": "7b80f0b6-861b-472f-b377-838cba0d8745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "741\n",
            "741\n",
            "<class 'list'>\n",
            "[array([[    203739,          0,          0, ...,     384866,          0,\n",
            "                 0],\n",
            "       [         0,   64585076,  139407692, ...,          0,  795064600,\n",
            "          14669396],\n",
            "       [         0,  139407692,  300913319, ...,          0, 1716157120,\n",
            "          31664082],\n",
            "       ...,\n",
            "       [    384866,          0,          0, ...,     727064,          0,\n",
            "                 0],\n",
            "       [         0,  795064600, 1716157120, ...,          0, 9787521566,\n",
            "         180585360],\n",
            "       [         0,   14669396,   31664082, ...,          0,  180585360,\n",
            "           3331976]]), array([[          0,  1241771116,  2680378862, ...,           0,\n",
            "        15286633852,   282047156],\n",
            "       [          0,  1145523532,  2472627229, ...,           0,\n",
            "        14101792652,   260186212],\n",
            "       [  118473569,           0,           0, ...,   223809756,\n",
            "                  0,           0],\n",
            "       ...,\n",
            "       [   23446973,           0,           0, ...,    44293942,\n",
            "                  0,           0],\n",
            "       [          0,     8238510,    17782930, ...,           0,\n",
            "          101418920,     1871240],\n",
            "       [   22633016,           0,           0, ...,    42756284,\n",
            "                  0,           0]]), array([2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
            "       1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
            "       1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
            "       1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
            "       1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
            "       2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
            "       2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2,\n",
            "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "       2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2,\n",
            "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1]), array([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
            "       2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
            "       2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "       1, 1, 2, 1, 2])]\n",
            "ACC: 1.0\n",
            "ROC: 1.0\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Employ the random forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "k_matrices = wl_kernel_fit_transform(G,y)\n",
        "clf = RandomForestClassifier(max_depth=5, random_state=42)\n",
        "clf.fit(k_matrices[0], k_matrices[2])\n",
        "\n",
        "y_prediction = clf.predict(k_matrices[1])\n",
        "\n",
        "# print the classification accuracy\n",
        "from sklearn import metrics\n",
        "\n",
        "print(\"ACC: {}\".format(accuracy_score(k_matrices[3], y_prediction)))\n",
        "print(\"ROC: {}\".format(roc_auc_score(k_matrices[3], y_prediction)))\n",
        "print(\"F1: {}\".format(f1_score(k_matrices[3], y_prediction)))\n",
        "print(\"Precision: {}\".format(precision_score(k_matrices[3], y_prediction)))\n",
        "print(\"Recall: {}\\n\".format(recall_score(k_matrices[3], y_prediction)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kQGW0skkdXh",
        "outputId": "2d3b08ae-4ce0-482c-e9e5-1aa5bde31ba6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "741\n",
            "741\n",
            "<class 'list'>\n",
            "[array([[    203739,          0,          0, ...,     384866,          0,\n",
            "                 0],\n",
            "       [         0,   64585076,  139407692, ...,          0,  795064600,\n",
            "          14669396],\n",
            "       [         0,  139407692,  300913319, ...,          0, 1716157120,\n",
            "          31664082],\n",
            "       ...,\n",
            "       [    384866,          0,          0, ...,     727064,          0,\n",
            "                 0],\n",
            "       [         0,  795064600, 1716157120, ...,          0, 9787521566,\n",
            "         180585360],\n",
            "       [         0,   14669396,   31664082, ...,          0,  180585360,\n",
            "           3331976]]), array([[          0,  1241771116,  2680378862, ...,           0,\n",
            "        15286633852,   282047156],\n",
            "       [          0,  1145523532,  2472627229, ...,           0,\n",
            "        14101792652,   260186212],\n",
            "       [  118473569,           0,           0, ...,   223809756,\n",
            "                  0,           0],\n",
            "       ...,\n",
            "       [   23446973,           0,           0, ...,    44293942,\n",
            "                  0,           0],\n",
            "       [          0,     8238510,    17782930, ...,           0,\n",
            "          101418920,     1871240],\n",
            "       [   22633016,           0,           0, ...,    42756284,\n",
            "                  0,           0]]), array([2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
            "       1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
            "       1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
            "       1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
            "       1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
            "       2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
            "       2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2,\n",
            "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "       2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2,\n",
            "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1]), array([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
            "       2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
            "       2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "       1, 1, 2, 1, 2])]\n",
            "ACC: 1.0\n",
            "ROC: 1.0\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Employ the adaboost classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "k_matrices = wl_kernel_fit_transform(G,y)\n",
        "clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(k_matrices[0], k_matrices[2])\n",
        "\n",
        "y_prediction = clf.predict(k_matrices[1])\n",
        "\n",
        "# print the classification accuracy\n",
        "from sklearn import metrics\n",
        "\n",
        "accuracies = metrics.accuracy_score(k_matrices[3], y_prediction)\n",
        "print(\"ACC: {}\".format(accuracy_score(k_matrices[3], y_prediction)))\n",
        "print(\"ROC: {}\".format(roc_auc_score(k_matrices[3], y_prediction)))\n",
        "print(\"F1: {}\".format(f1_score(k_matrices[3], y_prediction)))\n",
        "print(\"Precision: {}\".format(precision_score(k_matrices[3], y_prediction)))\n",
        "print(\"Recall: {}\\n\".format(recall_score(k_matrices[3], y_prediction)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocY0jZ7yknJx",
        "outputId": "ef293f66-ba15-430a-99ff-5838d9a1cfbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "741\n",
            "741\n",
            "<class 'list'>\n",
            "[array([[    203739,          0,          0, ...,     384866,          0,\n",
            "                 0],\n",
            "       [         0,   64585076,  139407692, ...,          0,  795064600,\n",
            "          14669396],\n",
            "       [         0,  139407692,  300913319, ...,          0, 1716157120,\n",
            "          31664082],\n",
            "       ...,\n",
            "       [    384866,          0,          0, ...,     727064,          0,\n",
            "                 0],\n",
            "       [         0,  795064600, 1716157120, ...,          0, 9787521566,\n",
            "         180585360],\n",
            "       [         0,   14669396,   31664082, ...,          0,  180585360,\n",
            "           3331976]]), array([[          0,  1241771116,  2680378862, ...,           0,\n",
            "        15286633852,   282047156],\n",
            "       [          0,  1145523532,  2472627229, ...,           0,\n",
            "        14101792652,   260186212],\n",
            "       [  118473569,           0,           0, ...,   223809756,\n",
            "                  0,           0],\n",
            "       ...,\n",
            "       [   23446973,           0,           0, ...,    44293942,\n",
            "                  0,           0],\n",
            "       [          0,     8238510,    17782930, ...,           0,\n",
            "          101418920,     1871240],\n",
            "       [   22633016,           0,           0, ...,    42756284,\n",
            "                  0,           0]]), array([2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
            "       1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
            "       1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
            "       1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
            "       1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
            "       2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
            "       2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2,\n",
            "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "       2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2,\n",
            "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1]), array([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
            "       2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
            "       2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "       1, 1, 2, 1, 2])]\n",
            "ACC: 1.0\n",
            "ROC: 1.0\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Employ the mlp classifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "k_matrices = wl_kernel_fit_transform(G,y)\n",
        "clf = MLPClassifier(random_state=42, max_iter=300)\n",
        "clf.fit(k_matrices[0], k_matrices[2])\n",
        "\n",
        "y_prediction = clf.predict(k_matrices[1])\n",
        "\n",
        "# print the classification accuracy\n",
        "from sklearn import metrics\n",
        "\n",
        "accuracies = metrics.accuracy_score(k_matrices[3], y_prediction)\n",
        "\n",
        "print(\"ACC: {}\".format(accuracy_score(k_matrices[3], y_prediction)))\n",
        "print(\"ROC: {}\".format(roc_auc_score(k_matrices[3], y_prediction)))\n",
        "print(\"F1: {}\".format(f1_score(k_matrices[3], y_prediction)))\n",
        "print(\"Precision: {}\".format(precision_score(k_matrices[3], y_prediction)))\n",
        "print(\"Recall: {}\\n\".format(recall_score(k_matrices[3], y_prediction)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFBA54A-_8y0",
        "outputId": "cde90346-f55a-4b3f-e5eb-3944f7325279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "741\n",
            "741\n",
            "<class 'list'>\n",
            "[array([[    203739,          0,          0, ...,     384866,          0,\n",
            "                 0],\n",
            "       [         0,   64585076,  139407692, ...,          0,  795064600,\n",
            "          14669396],\n",
            "       [         0,  139407692,  300913319, ...,          0, 1716157120,\n",
            "          31664082],\n",
            "       ...,\n",
            "       [    384866,          0,          0, ...,     727064,          0,\n",
            "                 0],\n",
            "       [         0,  795064600, 1716157120, ...,          0, 9787521566,\n",
            "         180585360],\n",
            "       [         0,   14669396,   31664082, ...,          0,  180585360,\n",
            "           3331976]]), array([[          0,  1241771116,  2680378862, ...,           0,\n",
            "        15286633852,   282047156],\n",
            "       [          0,  1145523532,  2472627229, ...,           0,\n",
            "        14101792652,   260186212],\n",
            "       [  118473569,           0,           0, ...,   223809756,\n",
            "                  0,           0],\n",
            "       ...,\n",
            "       [   23446973,           0,           0, ...,    44293942,\n",
            "                  0,           0],\n",
            "       [          0,     8238510,    17782930, ...,           0,\n",
            "          101418920,     1871240],\n",
            "       [   22633016,           0,           0, ...,    42756284,\n",
            "                  0,           0]]), array([2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
            "       1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
            "       1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
            "       1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
            "       1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
            "       2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
            "       2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2,\n",
            "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "       2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2,\n",
            "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1]), array([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
            "       2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
            "       2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "       1, 1, 2, 1, 2])]\n",
            "ACC: 0.9757085020242915\n",
            "ROC: 0.9761904761904762\n",
            "F1: 0.9758064516129031\n",
            "Precision: 0.952755905511811\n",
            "Recall: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Employ the GaussianNB classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "k_matrices = wl_kernel_fit_transform(G,y)\n",
        "clf = GaussianNB()\n",
        "clf.fit(k_matrices[0], k_matrices[2])\n",
        "\n",
        "y_prediction = clf.predict(k_matrices[1])\n",
        "\n",
        "# print the classification accuracy\n",
        "from sklearn import metrics\n",
        "\n",
        "accuracies = metrics.accuracy_score(k_matrices[3], y_prediction)\n",
        "\n",
        "print(\"ACC: {}\".format(accuracy_score(k_matrices[3], y_prediction)))\n",
        "print(\"ROC: {}\".format(roc_auc_score(k_matrices[3], y_prediction)))\n",
        "print(\"F1: {}\".format(f1_score(k_matrices[3], y_prediction)))\n",
        "print(\"Precision: {}\".format(precision_score(k_matrices[3], y_prediction)))\n",
        "print(\"Recall: {}\\n\".format(recall_score(k_matrices[3], y_prediction)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f4jp1qe_9S2",
        "outputId": "32278554-257d-481f-c5d0-6f5233eef49d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "741\n",
            "741\n",
            "<class 'list'>\n",
            "[array([[    203739,          0,          0, ...,     384866,          0,\n",
            "                 0],\n",
            "       [         0,   64585076,  139407692, ...,          0,  795064600,\n",
            "          14669396],\n",
            "       [         0,  139407692,  300913319, ...,          0, 1716157120,\n",
            "          31664082],\n",
            "       ...,\n",
            "       [    384866,          0,          0, ...,     727064,          0,\n",
            "                 0],\n",
            "       [         0,  795064600, 1716157120, ...,          0, 9787521566,\n",
            "         180585360],\n",
            "       [         0,   14669396,   31664082, ...,          0,  180585360,\n",
            "           3331976]]), array([[          0,  1241771116,  2680378862, ...,           0,\n",
            "        15286633852,   282047156],\n",
            "       [          0,  1145523532,  2472627229, ...,           0,\n",
            "        14101792652,   260186212],\n",
            "       [  118473569,           0,           0, ...,   223809756,\n",
            "                  0,           0],\n",
            "       ...,\n",
            "       [   23446973,           0,           0, ...,    44293942,\n",
            "                  0,           0],\n",
            "       [          0,     8238510,    17782930, ...,           0,\n",
            "          101418920,     1871240],\n",
            "       [   22633016,           0,           0, ...,    42756284,\n",
            "                  0,           0]]), array([2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
            "       1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
            "       1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
            "       1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
            "       1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
            "       2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
            "       2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2,\n",
            "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "       2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2,\n",
            "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1]), array([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
            "       2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
            "       2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "       1, 1, 2, 1, 2])]\n",
            "ACC: 0.9919028340080972\n",
            "ROC: 0.9920634920634921\n",
            "F1: 0.9918032786885246\n",
            "Precision: 0.983739837398374\n",
            "Recall: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Employ the knn classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "k_matrices = wl_kernel_fit_transform(G,y)\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "clf.fit(k_matrices[0], k_matrices[2])\n",
        "\n",
        "y_prediction = clf.predict(k_matrices[1])\n",
        "\n",
        "# print the classification accuracy\n",
        "from sklearn import metrics\n",
        "\n",
        "accuracies = metrics.accuracy_score(k_matrices[3], y_prediction)\n",
        "\n",
        "print(\"ACC: {}\".format(accuracy_score(k_matrices[3], y_prediction)))\n",
        "print(\"ROC: {}\".format(roc_auc_score(k_matrices[3], y_prediction)))\n",
        "print(\"F1: {}\".format(f1_score(k_matrices[3], y_prediction)))\n",
        "print(\"Precision: {}\".format(precision_score(k_matrices[3], y_prediction)))\n",
        "print(\"Recall: {}\\n\".format(recall_score(k_matrices[3], y_prediction)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x57oHzWa__3s",
        "outputId": "4e3138a0-a783-4a7e-c383-79ec766b7959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "741\n",
            "741\n",
            "<class 'list'>\n",
            "[array([[    203739,          0,          0, ...,     384866,          0,\n",
            "                 0],\n",
            "       [         0,   64585076,  139407692, ...,          0,  795064600,\n",
            "          14669396],\n",
            "       [         0,  139407692,  300913319, ...,          0, 1716157120,\n",
            "          31664082],\n",
            "       ...,\n",
            "       [    384866,          0,          0, ...,     727064,          0,\n",
            "                 0],\n",
            "       [         0,  795064600, 1716157120, ...,          0, 9787521566,\n",
            "         180585360],\n",
            "       [         0,   14669396,   31664082, ...,          0,  180585360,\n",
            "           3331976]]), array([[          0,  1241771116,  2680378862, ...,           0,\n",
            "        15286633852,   282047156],\n",
            "       [          0,  1145523532,  2472627229, ...,           0,\n",
            "        14101792652,   260186212],\n",
            "       [  118473569,           0,           0, ...,   223809756,\n",
            "                  0,           0],\n",
            "       ...,\n",
            "       [   23446973,           0,           0, ...,    44293942,\n",
            "                  0,           0],\n",
            "       [          0,     8238510,    17782930, ...,           0,\n",
            "          101418920,     1871240],\n",
            "       [   22633016,           0,           0, ...,    42756284,\n",
            "                  0,           0]]), array([2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
            "       1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
            "       1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
            "       1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
            "       1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
            "       2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
            "       2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2,\n",
            "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "       2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2,\n",
            "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1]), array([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
            "       2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
            "       2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "       1, 1, 2, 1, 2])]\n",
            "ACC: 1.0\n",
            "ROC: 1.0\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Employ the DecisionTreeClassifier classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "k_matrices = wl_kernel_fit_transform(G,y)\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(k_matrices[0], k_matrices[2])\n",
        "\n",
        "y_prediction = clf.predict(k_matrices[1])\n",
        "\n",
        "# print the classification accuracy\n",
        "from sklearn import metrics\n",
        "\n",
        "accuracies = metrics.accuracy_score(k_matrices[3], y_prediction)\n",
        "\n",
        "print(\"ACC: {}\".format(accuracy_score(k_matrices[3], y_prediction)))\n",
        "print(\"ROC: {}\".format(roc_auc_score(k_matrices[3], y_prediction)))\n",
        "print(\"F1: {}\".format(f1_score(k_matrices[3], y_prediction)))\n",
        "print(\"Precision: {}\".format(precision_score(k_matrices[3], y_prediction)))\n",
        "print(\"Recall: {}\\n\".format(recall_score(k_matrices[3], y_prediction)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tviaeB2cAB-3",
        "outputId": "8f5742c2-7047-44b1-fe48-881bdb2e7964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "741\n",
            "741\n",
            "<class 'list'>\n",
            "[array([[    203739,          0,          0, ...,     384866,          0,\n",
            "                 0],\n",
            "       [         0,   64585076,  139407692, ...,          0,  795064600,\n",
            "          14669396],\n",
            "       [         0,  139407692,  300913319, ...,          0, 1716157120,\n",
            "          31664082],\n",
            "       ...,\n",
            "       [    384866,          0,          0, ...,     727064,          0,\n",
            "                 0],\n",
            "       [         0,  795064600, 1716157120, ...,          0, 9787521566,\n",
            "         180585360],\n",
            "       [         0,   14669396,   31664082, ...,          0,  180585360,\n",
            "           3331976]]), array([[          0,  1241771116,  2680378862, ...,           0,\n",
            "        15286633852,   282047156],\n",
            "       [          0,  1145523532,  2472627229, ...,           0,\n",
            "        14101792652,   260186212],\n",
            "       [  118473569,           0,           0, ...,   223809756,\n",
            "                  0,           0],\n",
            "       ...,\n",
            "       [   23446973,           0,           0, ...,    44293942,\n",
            "                  0,           0],\n",
            "       [          0,     8238510,    17782930, ...,           0,\n",
            "          101418920,     1871240],\n",
            "       [   22633016,           0,           0, ...,    42756284,\n",
            "                  0,           0]]), array([2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
            "       1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
            "       1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
            "       1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
            "       1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
            "       2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
            "       2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2,\n",
            "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "       2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2,\n",
            "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1]), array([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
            "       2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
            "       2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "       1, 1, 2, 1, 2])]\n",
            "ACC: 1.0\n",
            "ROC: 1.0\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Employ the LogisticRegression classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "k_matrices = wl_kernel_fit_transform(G,y)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(k_matrices[0], k_matrices[2])\n",
        "\n",
        "y_prediction = clf.predict(k_matrices[1])\n",
        "\n",
        "# print the classification accuracy\n",
        "from sklearn import metrics\n",
        "\n",
        "accuracies = metrics.accuracy_score(k_matrices[3], y_prediction)\n",
        "\n",
        "print(\"ACC: {}\".format(accuracy_score(k_matrices[3], y_prediction)))\n",
        "print(\"ROC: {}\".format(roc_auc_score(k_matrices[3], y_prediction)))\n",
        "print(\"F1: {}\".format(f1_score(k_matrices[3], y_prediction)))\n",
        "print(\"Precision: {}\".format(precision_score(k_matrices[3], y_prediction)))\n",
        "print(\"Recall: {}\\n\".format(recall_score(k_matrices[3], y_prediction)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WM4Q5s37uLk",
        "outputId": "fe052003-b123-4307-b951-85a2d623d474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "741\n",
            "741\n",
            "<class 'list'>\n",
            "[array([[    203739,          0,          0, ...,     384866,          0,\n",
            "                 0],\n",
            "       [         0,   64585076,  139407692, ...,          0,  795064600,\n",
            "          14669396],\n",
            "       [         0,  139407692,  300913319, ...,          0, 1716157120,\n",
            "          31664082],\n",
            "       ...,\n",
            "       [    384866,          0,          0, ...,     727064,          0,\n",
            "                 0],\n",
            "       [         0,  795064600, 1716157120, ...,          0, 9787521566,\n",
            "         180585360],\n",
            "       [         0,   14669396,   31664082, ...,          0,  180585360,\n",
            "           3331976]]), array([[          0,  1241771116,  2680378862, ...,           0,\n",
            "        15286633852,   282047156],\n",
            "       [          0,  1145523532,  2472627229, ...,           0,\n",
            "        14101792652,   260186212],\n",
            "       [  118473569,           0,           0, ...,   223809756,\n",
            "                  0,           0],\n",
            "       ...,\n",
            "       [   23446973,           0,           0, ...,    44293942,\n",
            "                  0,           0],\n",
            "       [          0,     8238510,    17782930, ...,           0,\n",
            "          101418920,     1871240],\n",
            "       [   22633016,           0,           0, ...,    42756284,\n",
            "                  0,           0]]), array([2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
            "       1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
            "       1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
            "       1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
            "       1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
            "       2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
            "       2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2,\n",
            "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "       2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2,\n",
            "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1]), array([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
            "       2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
            "       2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "       1, 1, 2, 1, 2])]\n",
            "ACC: 1.0\n",
            "ROC: 1.0\n",
            "F1: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "k_matrices = wl_kernel_fit_transform(G,y)\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(k_matrices[0], k_matrices[2])\n",
        "\n",
        "y_prediction = clf.predict(k_matrices[1])\n",
        "\n",
        "# print the classification accuracy\n",
        "from sklearn import metrics\n",
        "\n",
        "accuracies = metrics.accuracy_score(k_matrices[3], y_prediction)\n",
        "\n",
        "print(\"ACC: {}\".format(accuracy_score(k_matrices[3], y_prediction)))\n",
        "print(\"ROC: {}\".format(roc_auc_score(k_matrices[3], y_prediction)))\n",
        "print(\"F1: {}\".format(f1_score(k_matrices[3], y_prediction)))\n",
        "print(\"Precision: {}\".format(precision_score(k_matrices[3], y_prediction)))\n",
        "print(\"Recall: {}\\n\".format(recall_score(k_matrices[3], y_prediction)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWCm6fJ98Aqy",
        "outputId": "ffc2da2a-3673-4b42-fe41-7c7a9a86d1ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "741\n",
            "741\n",
            "<class 'list'>\n",
            "[array([[    203739,          0,          0, ...,     384866,          0,\n",
            "                 0],\n",
            "       [         0,   64585076,  139407692, ...,          0,  795064600,\n",
            "          14669396],\n",
            "       [         0,  139407692,  300913319, ...,          0, 1716157120,\n",
            "          31664082],\n",
            "       ...,\n",
            "       [    384866,          0,          0, ...,     727064,          0,\n",
            "                 0],\n",
            "       [         0,  795064600, 1716157120, ...,          0, 9787521566,\n",
            "         180585360],\n",
            "       [         0,   14669396,   31664082, ...,          0,  180585360,\n",
            "           3331976]]), array([[          0,  1241771116,  2680378862, ...,           0,\n",
            "        15286633852,   282047156],\n",
            "       [          0,  1145523532,  2472627229, ...,           0,\n",
            "        14101792652,   260186212],\n",
            "       [  118473569,           0,           0, ...,   223809756,\n",
            "                  0,           0],\n",
            "       ...,\n",
            "       [   23446973,           0,           0, ...,    44293942,\n",
            "                  0,           0],\n",
            "       [          0,     8238510,    17782930, ...,           0,\n",
            "          101418920,     1871240],\n",
            "       [   22633016,           0,           0, ...,    42756284,\n",
            "                  0,           0]]), array([2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2,\n",
            "       1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
            "       1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
            "       1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2,\n",
            "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
            "       1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
            "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
            "       2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
            "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2,\n",
            "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1,\n",
            "       1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
            "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
            "       2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2,\n",
            "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
            "       2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2,\n",
            "       2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2,\n",
            "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
            "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
            "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1]), array([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
            "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2,\n",
            "       1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
            "       2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
            "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1,\n",
            "       2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
            "       2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
            "       2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1,\n",
            "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
            "       1, 1, 2, 1, 2])]\n",
            "ACC: 0.8744939271255061\n",
            "ROC: 0.876984126984127\n",
            "F1: 0.8864468864468864\n",
            "Precision: 0.7960526315789473\n",
            "Recall: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "k_matrices = wl_kernel_fit_transform(G,y)\n",
        "clf = SVC(random_state=42)\n",
        "clf.fit(k_matrices[0], k_matrices[2])\n",
        "\n",
        "y_prediction = clf.predict(k_matrices[1])\n",
        "\n",
        "# print the classification accuracy\n",
        "from sklearn import metrics\n",
        "\n",
        "accuracies = metrics.accuracy_score(k_matrices[3], y_prediction)\n",
        "\n",
        "print(\"ACC: {}\".format(accuracy_score(k_matrices[3], y_prediction)))\n",
        "print(\"ROC: {}\".format(roc_auc_score(k_matrices[3], y_prediction)))\n",
        "print(\"F1: {}\".format(f1_score(k_matrices[3], y_prediction)))\n",
        "print(\"Precision: {}\".format(precision_score(k_matrices[3], y_prediction)))\n",
        "print(\"Recall: {}\\n\".format(recall_score(k_matrices[3], y_prediction)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "4b432cacd44a0b9728669a2b4b9095acc348578df38561d3d67f70f5327bdf55"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
